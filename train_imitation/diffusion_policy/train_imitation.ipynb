{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "success_rate, time_taken, distance_travelled, straight_distance, (optimal_time or dijsktra_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>success</th>\n",
       "      <th>actual_time</th>\n",
       "      <th>optimal_time</th>\n",
       "      <th>world_idx</th>\n",
       "      <th>timestep</th>\n",
       "      <th>lidar_0</th>\n",
       "      <th>lidar_1</th>\n",
       "      <th>lidar_2</th>\n",
       "      <th>lidar_3</th>\n",
       "      <th>lidar_4</th>\n",
       "      <th>...</th>\n",
       "      <th>lidar_719</th>\n",
       "      <th>pos_x</th>\n",
       "      <th>pos_y</th>\n",
       "      <th>pose_heading</th>\n",
       "      <th>twist_linear</th>\n",
       "      <th>twist_angular</th>\n",
       "      <th>cmd_vel_linear</th>\n",
       "      <th>cmd_vel_angular</th>\n",
       "      <th>local_goal_x</th>\n",
       "      <th>local_goal_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974557</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.006651</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974500</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.970249</td>\n",
       "      <td>2.952848</td>\n",
       "      <td>2.942288</td>\n",
       "      <td>2.937646</td>\n",
       "      <td>2.942004</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974500</td>\n",
       "      <td>-0.000416</td>\n",
       "      <td>0.006752</td>\n",
       "      <td>1.571106</td>\n",
       "      <td>0.054851</td>\n",
       "      <td>-0.000015</td>\n",
       "      <td>0.066</td>\n",
       "      <td>0.007315</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.970935</td>\n",
       "      <td>2.952720</td>\n",
       "      <td>2.942559</td>\n",
       "      <td>2.940268</td>\n",
       "      <td>2.943001</td>\n",
       "      <td>...</td>\n",
       "      <td>2.972595</td>\n",
       "      <td>-0.000417</td>\n",
       "      <td>0.008014</td>\n",
       "      <td>1.571247</td>\n",
       "      <td>0.060888</td>\n",
       "      <td>0.008210</td>\n",
       "      <td>0.072</td>\n",
       "      <td>0.007980</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>True</td>\n",
       "      <td>12.367</td>\n",
       "      <td>6.796149</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.969906</td>\n",
       "      <td>2.953190</td>\n",
       "      <td>2.943104</td>\n",
       "      <td>2.942191</td>\n",
       "      <td>2.944316</td>\n",
       "      <td>...</td>\n",
       "      <td>2.972554</td>\n",
       "      <td>-0.000418</td>\n",
       "      <td>0.009397</td>\n",
       "      <td>1.571141</td>\n",
       "      <td>0.066950</td>\n",
       "      <td>-0.001348</td>\n",
       "      <td>0.078</td>\n",
       "      <td>0.008645</td>\n",
       "      <td>-0.05</td>\n",
       "      <td>0.35</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 734 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   success  actual_time  optimal_time  world_idx  timestep   lidar_0  \\\n",
       "0     True       12.367      6.796149          0         0  2.970249   \n",
       "1     True       12.367      6.796149          0         1  2.970249   \n",
       "2     True       12.367      6.796149          0         2  2.970249   \n",
       "3     True       12.367      6.796149          0         3  2.970935   \n",
       "4     True       12.367      6.796149          0         4  2.969906   \n",
       "\n",
       "    lidar_1   lidar_2   lidar_3   lidar_4  ...  lidar_719     pos_x     pos_y  \\\n",
       "0  2.952848  2.942288  2.937646  2.942004  ...   2.974557 -0.000416  0.006752   \n",
       "1  2.952848  2.942288  2.937646  2.942004  ...   2.974500 -0.000416  0.006752   \n",
       "2  2.952848  2.942288  2.937646  2.942004  ...   2.974500 -0.000416  0.006752   \n",
       "3  2.952720  2.942559  2.940268  2.943001  ...   2.972595 -0.000417  0.008014   \n",
       "4  2.953190  2.943104  2.942191  2.944316  ...   2.972554 -0.000418  0.009397   \n",
       "\n",
       "   pose_heading  twist_linear  twist_angular  cmd_vel_linear  cmd_vel_angular  \\\n",
       "0      1.571106      0.054851      -0.000015           0.066         0.006651   \n",
       "1      1.571106      0.054851      -0.000015           0.066         0.007315   \n",
       "2      1.571106      0.054851      -0.000015           0.066         0.007315   \n",
       "3      1.571247      0.060888       0.008210           0.072         0.007980   \n",
       "4      1.571141      0.066950      -0.001348           0.078         0.008645   \n",
       "\n",
       "   local_goal_x  local_goal_y  \n",
       "0         -0.05          0.35  \n",
       "1         -0.05          0.35  \n",
       "2         -0.05          0.35  \n",
       "3         -0.05          0.35  \n",
       "4         -0.05          0.35  \n",
       "\n",
       "[5 rows x 734 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('data_sorted.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual_time</th>\n",
       "      <th>optimal_time</th>\n",
       "      <th>world_idx</th>\n",
       "      <th>timestep</th>\n",
       "      <th>lidar_0</th>\n",
       "      <th>lidar_1</th>\n",
       "      <th>lidar_2</th>\n",
       "      <th>lidar_3</th>\n",
       "      <th>lidar_4</th>\n",
       "      <th>lidar_5</th>\n",
       "      <th>...</th>\n",
       "      <th>lidar_719</th>\n",
       "      <th>pos_x</th>\n",
       "      <th>pos_y</th>\n",
       "      <th>pose_heading</th>\n",
       "      <th>twist_linear</th>\n",
       "      <th>twist_angular</th>\n",
       "      <th>cmd_vel_linear</th>\n",
       "      <th>cmd_vel_angular</th>\n",
       "      <th>local_goal_x</th>\n",
       "      <th>local_goal_y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "      <td>200540.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>12.731146</td>\n",
       "      <td>5.705953</td>\n",
       "      <td>151.868460</td>\n",
       "      <td>340.027182</td>\n",
       "      <td>2.977978</td>\n",
       "      <td>2.973894</td>\n",
       "      <td>2.970005</td>\n",
       "      <td>2.966131</td>\n",
       "      <td>2.962435</td>\n",
       "      <td>2.956835</td>\n",
       "      <td>...</td>\n",
       "      <td>2.974177</td>\n",
       "      <td>-0.031480</td>\n",
       "      <td>4.134166</td>\n",
       "      <td>1.554468</td>\n",
       "      <td>0.709279</td>\n",
       "      <td>-0.001472</td>\n",
       "      <td>0.710038</td>\n",
       "      <td>-0.001961</td>\n",
       "      <td>-0.048144</td>\n",
       "      <td>4.451120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2.094962</td>\n",
       "      <td>0.366456</td>\n",
       "      <td>86.482099</td>\n",
       "      <td>205.708619</td>\n",
       "      <td>2.099984</td>\n",
       "      <td>2.112733</td>\n",
       "      <td>2.124795</td>\n",
       "      <td>2.136720</td>\n",
       "      <td>2.148487</td>\n",
       "      <td>2.159736</td>\n",
       "      <td>...</td>\n",
       "      <td>2.181376</td>\n",
       "      <td>0.524510</td>\n",
       "      <td>2.795738</td>\n",
       "      <td>0.346113</td>\n",
       "      <td>0.205768</td>\n",
       "      <td>0.248443</td>\n",
       "      <td>0.202546</td>\n",
       "      <td>0.219348</td>\n",
       "      <td>0.536361</td>\n",
       "      <td>2.801547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>11.778000</td>\n",
       "      <td>5.026614</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.316017</td>\n",
       "      <td>0.312385</td>\n",
       "      <td>0.308484</td>\n",
       "      <td>0.304327</td>\n",
       "      <td>0.302636</td>\n",
       "      <td>0.299997</td>\n",
       "      <td>...</td>\n",
       "      <td>0.291395</td>\n",
       "      <td>-1.710844</td>\n",
       "      <td>-0.000102</td>\n",
       "      <td>-3.141033</td>\n",
       "      <td>-0.317507</td>\n",
       "      <td>-2.412546</td>\n",
       "      <td>-0.300000</td>\n",
       "      <td>-1.570796</td>\n",
       "      <td>-1.814288</td>\n",
       "      <td>0.293437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.923000</td>\n",
       "      <td>5.441711</td>\n",
       "      <td>78.000000</td>\n",
       "      <td>167.000000</td>\n",
       "      <td>1.693966</td>\n",
       "      <td>1.688156</td>\n",
       "      <td>1.682930</td>\n",
       "      <td>1.677800</td>\n",
       "      <td>1.673200</td>\n",
       "      <td>1.668052</td>\n",
       "      <td>...</td>\n",
       "      <td>1.612400</td>\n",
       "      <td>-0.294785</td>\n",
       "      <td>1.631851</td>\n",
       "      <td>1.467639</td>\n",
       "      <td>0.799555</td>\n",
       "      <td>-0.059794</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>-0.065792</td>\n",
       "      <td>-0.325890</td>\n",
       "      <td>1.919969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>12.121000</td>\n",
       "      <td>5.612314</td>\n",
       "      <td>153.000000</td>\n",
       "      <td>335.000000</td>\n",
       "      <td>2.879627</td>\n",
       "      <td>2.865375</td>\n",
       "      <td>2.851746</td>\n",
       "      <td>2.838104</td>\n",
       "      <td>2.824749</td>\n",
       "      <td>2.811204</td>\n",
       "      <td>...</td>\n",
       "      <td>2.844913</td>\n",
       "      <td>-0.004195</td>\n",
       "      <td>4.014582</td>\n",
       "      <td>1.579035</td>\n",
       "      <td>0.800235</td>\n",
       "      <td>-0.000544</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.000088</td>\n",
       "      <td>-0.050000</td>\n",
       "      <td>4.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>12.502000</td>\n",
       "      <td>5.891979</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>503.000000</td>\n",
       "      <td>3.187622</td>\n",
       "      <td>3.172247</td>\n",
       "      <td>3.157621</td>\n",
       "      <td>3.144867</td>\n",
       "      <td>3.130252</td>\n",
       "      <td>3.113974</td>\n",
       "      <td>...</td>\n",
       "      <td>3.177820</td>\n",
       "      <td>0.217379</td>\n",
       "      <td>6.539444</td>\n",
       "      <td>1.680124</td>\n",
       "      <td>0.801052</td>\n",
       "      <td>0.044571</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>0.052247</td>\n",
       "      <td>0.231511</td>\n",
       "      <td>6.875039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>25.666000</td>\n",
       "      <td>6.867653</td>\n",
       "      <td>299.000000</td>\n",
       "      <td>1328.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>1.690635</td>\n",
       "      <td>9.312726</td>\n",
       "      <td>3.140765</td>\n",
       "      <td>0.810490</td>\n",
       "      <td>2.206750</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>1.570796</td>\n",
       "      <td>1.689592</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 733 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         actual_time   optimal_time      world_idx       timestep  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean       12.731146       5.705953     151.868460     340.027182   \n",
       "std         2.094962       0.366456      86.482099     205.708619   \n",
       "min        11.778000       5.026614       0.000000       0.000000   \n",
       "25%        11.923000       5.441711      78.000000     167.000000   \n",
       "50%        12.121000       5.612314     153.000000     335.000000   \n",
       "75%        12.502000       5.891979     227.000000     503.000000   \n",
       "max        25.666000       6.867653     299.000000    1328.000000   \n",
       "\n",
       "             lidar_0        lidar_1        lidar_2        lidar_3  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean        2.977978       2.973894       2.970005       2.966131   \n",
       "std         2.099984       2.112733       2.124795       2.136720   \n",
       "min         0.316017       0.312385       0.308484       0.304327   \n",
       "25%         1.693966       1.688156       1.682930       1.677800   \n",
       "50%         2.879627       2.865375       2.851746       2.838104   \n",
       "75%         3.187622       3.172247       3.157621       3.144867   \n",
       "max        10.000000      10.000000      10.000000      10.000000   \n",
       "\n",
       "             lidar_4        lidar_5  ...      lidar_719          pos_x  \\\n",
       "count  200540.000000  200540.000000  ...  200540.000000  200540.000000   \n",
       "mean        2.962435       2.956835  ...       2.974177      -0.031480   \n",
       "std         2.148487       2.159736  ...       2.181376       0.524510   \n",
       "min         0.302636       0.299997  ...       0.291395      -1.710844   \n",
       "25%         1.673200       1.668052  ...       1.612400      -0.294785   \n",
       "50%         2.824749       2.811204  ...       2.844913      -0.004195   \n",
       "75%         3.130252       3.113974  ...       3.177820       0.217379   \n",
       "max        10.000000      10.000000  ...      10.000000       1.690635   \n",
       "\n",
       "               pos_y   pose_heading   twist_linear  twist_angular  \\\n",
       "count  200540.000000  200540.000000  200540.000000  200540.000000   \n",
       "mean        4.134166       1.554468       0.709279      -0.001472   \n",
       "std         2.795738       0.346113       0.205768       0.248443   \n",
       "min        -0.000102      -3.141033      -0.317507      -2.412546   \n",
       "25%         1.631851       1.467639       0.799555      -0.059794   \n",
       "50%         4.014582       1.579035       0.800235      -0.000544   \n",
       "75%         6.539444       1.680124       0.801052       0.044571   \n",
       "max         9.312726       3.140765       0.810490       2.206750   \n",
       "\n",
       "       cmd_vel_linear  cmd_vel_angular   local_goal_x   local_goal_y  \n",
       "count   200540.000000    200540.000000  200540.000000  200540.000000  \n",
       "mean         0.710038        -0.001961      -0.048144       4.451120  \n",
       "std          0.202546         0.219348       0.536361       2.801547  \n",
       "min         -0.300000        -1.570796      -1.814288       0.293437  \n",
       "25%          0.800000        -0.065792      -0.325890       1.919969  \n",
       "50%          0.800000         0.000088      -0.050000       4.300000  \n",
       "75%          0.800000         0.052247       0.231511       6.875039  \n",
       "max          0.800000         1.570796       1.689592      10.000000  \n",
       "\n",
       "[8 rows x 733 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch Dataset\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "class KULBarnDataset(Dataset):\n",
    "    def get_local_goal(self):\n",
    "        x = self.data['pos_x']\n",
    "        y = self.data['pos_y']\n",
    "        theta = self.data['pose_heading']\n",
    "        goal_x = self.data['local_goal_x']\n",
    "        goal_y = self.data['local_goal_y']\n",
    "        self.data['local_x'] = (goal_x - x) * np.cos(theta) + (goal_y - y) * np.sin(theta)\n",
    "        self.data['local_y'] = -(goal_x - x) * np.sin(theta) + (goal_y - y) * np.cos(theta)\n",
    "        self.data['distance'] = np.sqrt((goal_x - x)**2 + (goal_y - y)**2)\n",
    "        self.data['local_x'] /= self.data['distance']\n",
    "        self.data['local_y'] /= self.data['distance']\n",
    "    \n",
    "    def __init__(self, df, mode=\"train\"):\n",
    "        super().__init__()\n",
    "\n",
    "        self.data = df\n",
    "        self.get_local_goal()  \n",
    "        \n",
    "        self.data = self.data.drop(columns=[\n",
    "            'world_idx', 'timestep', 'actual_time', 'optimal_time', 'success'\n",
    "        ])\n",
    "\n",
    "        # get all the column values that contain the word lidar\n",
    "        self.lidar_cols = [\"lidar_\" + str(i) for i in range(0, 720, 1)]\n",
    "        # get actions columns\n",
    "        self.actions_cols = ['cmd_vel_linear', 'cmd_vel_angular']\n",
    "        # get other columns\n",
    "        self.non_lidar_cols = ['local_x', 'local_y']\n",
    "\n",
    "        if mode == \"train\":\n",
    "            # Manually compute the min and max values for each column\n",
    "            self.min = self.data.min()\n",
    "            self.max = self.data.max()\n",
    "\n",
    "            # Save the mean and std to a JSON file\n",
    "            scaler_params = {\n",
    "                'min': self.min.to_dict(),\n",
    "                'max': self.max.to_dict()\n",
    "            }\n",
    "            with open('scaler_params.json', 'w') as f:\n",
    "                json.dump(scaler_params, f)\n",
    "        else:\n",
    "            # Load the mean and std from the JSON file\n",
    "            with open('scaler_params.json', 'r') as f:\n",
    "                scaler_params = json.load(f)\n",
    "            self.min = pd.Series(scaler_params['min'])\n",
    "            self.max = pd.Series(scaler_params['max'])\n",
    "        \n",
    "        # dont normalizer local_x and local_y\n",
    "        self.normalized_data = (self.data - self.min) / (self.max - self.min)\n",
    "        \n",
    "        self.lidar_data = self.normalized_data[self.lidar_cols].values\n",
    "        self.non_lidar_data = self.normalized_data[self.non_lidar_cols].values\n",
    "        self.actions_data = self.normalized_data[self.actions_cols].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        lidar = self.lidar_data[idx]\n",
    "        non_lidar = self.non_lidar_data[idx]\n",
    "        actions = self.actions_data[idx]\n",
    "        return lidar, non_lidar, actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split train and test\n",
    "import random\n",
    "# set random seed\n",
    "random.seed(42)\n",
    "\n",
    "NO_WORLDS = 300\n",
    "TRAIN_RATIO = 0.7\n",
    "VAL_RATIO = 0.1\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "world_ids = [i for i in range(NO_WORLDS)]\n",
    "test_ids = [id for id in range(0, NO_WORLDS, 5)]\n",
    "train_evals = [id for id in world_ids if id not in test_ids]\n",
    "train_ids = random.sample(train_evals, int(NO_WORLDS * TRAIN_RATIO))\n",
    "val_ids = [id for id in train_evals if id not in train_ids]\n",
    "\n",
    "train_df = df[df['world_idx'].isin(train_ids)]\n",
    "val_df = df[df['world_idx'].isin(val_ids)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210\n",
      "30\n",
      "60\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ids))\n",
    "print(len(val_ids))\n",
    "print(len(test_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 27, 37, 56, 57, 83, 98, 106, 107, 132, 142, 144, 149, 153, 154, 158, 166, 176, 187, 194, 221, 238, 242, 247, 253, 258, 262, 278, 286, 289]\n"
     ]
    }
   ],
   "source": [
    "print(val_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "140392\n",
      "19838\n"
     ]
    }
   ],
   "source": [
    "print(len(train_df))\n",
    "print(len(val_df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = KULBarnDataset(train_df, \"train\")\n",
    "val_dataset = KULBarnDataset(val_df, \"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Dataset Length: 140392\n",
      "Val Dataset Length: 19838\n"
     ]
    }
   ],
   "source": [
    "print(\"Train Dataset Length:\", len(train_dataset))\n",
    "print(\"Val Dataset Length:\", len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Non lidar shape: torch.Size([64, 2])\n",
      "Lidar shape: torch.Size([64, 720])\n",
      "Train loader size: 2194\n",
      "Val loader size: 310\n",
      "tensor([[0.1179, 0.1188, 0.1041,  ..., 0.0417, 0.0415, 0.0416],\n",
      "        [0.2740, 0.2725, 0.2719,  ..., 0.2734, 0.2742, 0.2760],\n",
      "        [0.5271, 0.5239, 0.5107,  ..., 0.0169, 0.0169, 0.0169],\n",
      "        ...,\n",
      "        [0.2819, 0.2825, 0.2833,  ..., 1.0000, 1.0000, 1.0000],\n",
      "        [0.2392, 0.2398, 0.2410,  ..., 0.2906, 0.2922, 0.3013],\n",
      "        [0.2722, 0.2730, 0.2652,  ..., 0.2841, 0.2838, 0.2840]],\n",
      "       dtype=torch.float64) tensor([[0.9568, 0.7034],\n",
      "        [0.9947, 0.5727],\n",
      "        [0.9908, 0.4047],\n",
      "        [1.0000, 0.5014],\n",
      "        [0.9923, 0.5873],\n",
      "        [1.0000, 0.5014],\n",
      "        [1.0000, 0.5026],\n",
      "        [0.9999, 0.5122],\n",
      "        [0.9996, 0.5189],\n",
      "        [0.9941, 0.4233],\n",
      "        [0.9995, 0.5218],\n",
      "        [0.9976, 0.5493],\n",
      "        [1.0000, 0.5044],\n",
      "        [0.9996, 0.5203],\n",
      "        [1.0000, 0.5017],\n",
      "        [1.0000, 0.5065],\n",
      "        [0.9977, 0.5481],\n",
      "        [0.9946, 0.5733],\n",
      "        [0.9964, 0.5598],\n",
      "        [0.9997, 0.5186],\n",
      "        [0.9989, 0.5324],\n",
      "        [0.9911, 0.5941],\n",
      "        [0.9981, 0.5439],\n",
      "        [1.0000, 0.4934],\n",
      "        [0.9997, 0.5177],\n",
      "        [1.0000, 0.5000],\n",
      "        [0.9752, 0.6556],\n",
      "        [0.9998, 0.5142],\n",
      "        [0.9992, 0.4719],\n",
      "        [0.9934, 0.5811],\n",
      "        [0.9875, 0.3887],\n",
      "        [0.9986, 0.5374],\n",
      "        [0.9994, 0.5246],\n",
      "        [0.9875, 0.6111],\n",
      "        [0.9997, 0.4831],\n",
      "        [0.9997, 0.5164],\n",
      "        [0.9999, 0.4907],\n",
      "        [0.9969, 0.4444],\n",
      "        [0.9941, 0.5766],\n",
      "        [0.9992, 0.5277],\n",
      "        [0.9930, 0.5835],\n",
      "        [0.9977, 0.5481],\n",
      "        [0.9972, 0.5532],\n",
      "        [1.0000, 0.5037],\n",
      "        [0.9986, 0.5379],\n",
      "        [0.9850, 0.6215],\n",
      "        [0.9999, 0.5088],\n",
      "        [0.9941, 0.5769],\n",
      "        [0.9999, 0.5120],\n",
      "        [0.5401, 0.9984],\n",
      "        [0.9669, 0.3211],\n",
      "        [0.9969, 0.5553],\n",
      "        [0.9271, 0.2401],\n",
      "        [0.9973, 0.5523],\n",
      "        [0.9997, 0.5169],\n",
      "        [0.9976, 0.5489],\n",
      "        [1.0000, 0.5022],\n",
      "        [0.9953, 0.5687],\n",
      "        [0.9709, 0.6681],\n",
      "        [0.9939, 0.5780],\n",
      "        [0.9999, 0.4923],\n",
      "        [0.9986, 0.5379],\n",
      "        [0.9995, 0.5230],\n",
      "        [0.9978, 0.5469]], dtype=torch.float64) tensor([[1.0000, 0.4597],\n",
      "        [0.3491, 0.5030],\n",
      "        [1.0000, 0.5286],\n",
      "        [1.0000, 0.5000],\n",
      "        [1.0000, 0.5003],\n",
      "        [0.5782, 0.4674],\n",
      "        [1.0000, 0.4632],\n",
      "        [1.0000, 0.5148],\n",
      "        [1.0000, 0.5014],\n",
      "        [1.0000, 0.3764],\n",
      "        [0.9109, 0.4694],\n",
      "        [0.8509, 0.5520],\n",
      "        [1.0000, 0.4972],\n",
      "        [1.0000, 0.5001],\n",
      "        [1.0000, 0.4768],\n",
      "        [1.0000, 0.4482],\n",
      "        [0.6655, 0.5431],\n",
      "        [1.0000, 0.4939],\n",
      "        [1.0000, 0.5629],\n",
      "        [1.0000, 0.5161],\n",
      "        [1.0000, 0.4705],\n",
      "        [1.0000, 0.5018],\n",
      "        [1.0000, 0.4584],\n",
      "        [1.0000, 0.4986],\n",
      "        [1.0000, 0.4928],\n",
      "        [1.0000, 0.5000],\n",
      "        [1.0000, 0.5003],\n",
      "        [1.0000, 0.5041],\n",
      "        [1.0000, 0.4901],\n",
      "        [1.0000, 0.5044],\n",
      "        [0.7364, 0.3847],\n",
      "        [1.0000, 0.4614],\n",
      "        [1.0000, 0.5335],\n",
      "        [1.0000, 0.5979],\n",
      "        [1.0000, 0.5143],\n",
      "        [1.0000, 0.4892],\n",
      "        [1.0000, 0.4962],\n",
      "        [1.0000, 0.3751],\n",
      "        [0.3164, 0.5015],\n",
      "        [1.0000, 0.4897],\n",
      "        [1.0000, 0.4461],\n",
      "        [1.0000, 0.4811],\n",
      "        [0.7473, 0.5075],\n",
      "        [1.0000, 0.5000],\n",
      "        [1.0000, 0.4642],\n",
      "        [1.0000, 0.5033],\n",
      "        [1.0000, 0.4910],\n",
      "        [1.0000, 0.4941],\n",
      "        [1.0000, 0.4624],\n",
      "        [0.3709, 0.7322],\n",
      "        [0.6600, 0.3587],\n",
      "        [1.0000, 0.5067],\n",
      "        [0.7764, 0.3086],\n",
      "        [0.7418, 0.5074],\n",
      "        [1.0000, 0.4321],\n",
      "        [1.0000, 0.4805],\n",
      "        [1.0000, 0.5285],\n",
      "        [1.0000, 0.5458],\n",
      "        [1.0000, 0.3038],\n",
      "        [1.0000, 0.4942],\n",
      "        [1.0000, 0.5069],\n",
      "        [1.0000, 0.4682],\n",
      "        [1.0000, 0.5025],\n",
      "        [0.8291, 0.5003]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# dataloader\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
    "# test dataloader\n",
    "lidar, non_lidar, actions = next(iter(train_loader))\n",
    "print(f\"Non lidar shape: {non_lidar.shape}\")\n",
    "print(f\"Lidar shape: {lidar.shape}\")\n",
    "# print size dataloader\n",
    "print(f\"Train loader size: {len(train_loader)}\")\n",
    "print(f\"Val loader size: {len(val_loader)}\")\n",
    "print(lidar, non_lidar, actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1179, 0.1188, 0.1041,  ..., 0.0417, 0.0415, 0.0416],\n",
       "        [0.2740, 0.2725, 0.2719,  ..., 0.2734, 0.2742, 0.2760],\n",
       "        [0.5271, 0.5239, 0.5107,  ..., 0.0169, 0.0169, 0.0169],\n",
       "        ...,\n",
       "        [0.2819, 0.2825, 0.2833,  ..., 1.0000, 1.0000, 1.0000],\n",
       "        [0.2392, 0.2398, 0.2410,  ..., 0.2906, 0.2922, 0.3013],\n",
       "        [0.2722, 0.2730, 0.2652,  ..., 0.2841, 0.2838, 0.2840]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9568, 0.7034],\n",
       "        [0.9947, 0.5727],\n",
       "        [0.9908, 0.4047],\n",
       "        [1.0000, 0.5014],\n",
       "        [0.9923, 0.5873],\n",
       "        [1.0000, 0.5014],\n",
       "        [1.0000, 0.5026],\n",
       "        [0.9999, 0.5122],\n",
       "        [0.9996, 0.5189],\n",
       "        [0.9941, 0.4233],\n",
       "        [0.9995, 0.5218],\n",
       "        [0.9976, 0.5493],\n",
       "        [1.0000, 0.5044],\n",
       "        [0.9996, 0.5203],\n",
       "        [1.0000, 0.5017],\n",
       "        [1.0000, 0.5065],\n",
       "        [0.9977, 0.5481],\n",
       "        [0.9946, 0.5733],\n",
       "        [0.9964, 0.5598],\n",
       "        [0.9997, 0.5186],\n",
       "        [0.9989, 0.5324],\n",
       "        [0.9911, 0.5941],\n",
       "        [0.9981, 0.5439],\n",
       "        [1.0000, 0.4934],\n",
       "        [0.9997, 0.5177],\n",
       "        [1.0000, 0.5000],\n",
       "        [0.9752, 0.6556],\n",
       "        [0.9998, 0.5142],\n",
       "        [0.9992, 0.4719],\n",
       "        [0.9934, 0.5811],\n",
       "        [0.9875, 0.3887],\n",
       "        [0.9986, 0.5374],\n",
       "        [0.9994, 0.5246],\n",
       "        [0.9875, 0.6111],\n",
       "        [0.9997, 0.4831],\n",
       "        [0.9997, 0.5164],\n",
       "        [0.9999, 0.4907],\n",
       "        [0.9969, 0.4444],\n",
       "        [0.9941, 0.5766],\n",
       "        [0.9992, 0.5277],\n",
       "        [0.9930, 0.5835],\n",
       "        [0.9977, 0.5481],\n",
       "        [0.9972, 0.5532],\n",
       "        [1.0000, 0.5037],\n",
       "        [0.9986, 0.5379],\n",
       "        [0.9850, 0.6215],\n",
       "        [0.9999, 0.5088],\n",
       "        [0.9941, 0.5769],\n",
       "        [0.9999, 0.5120],\n",
       "        [0.5401, 0.9984],\n",
       "        [0.9669, 0.3211],\n",
       "        [0.9969, 0.5553],\n",
       "        [0.9271, 0.2401],\n",
       "        [0.9973, 0.5523],\n",
       "        [0.9997, 0.5169],\n",
       "        [0.9976, 0.5489],\n",
       "        [1.0000, 0.5022],\n",
       "        [0.9953, 0.5687],\n",
       "        [0.9709, 0.6681],\n",
       "        [0.9939, 0.5780],\n",
       "        [0.9999, 0.4923],\n",
       "        [0.9986, 0.5379],\n",
       "        [0.9995, 0.5230],\n",
       "        [0.9978, 0.5469]], dtype=torch.float64)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "non_lidar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.4597],\n",
       "        [0.3491, 0.5030],\n",
       "        [1.0000, 0.5286],\n",
       "        [1.0000, 0.5000],\n",
       "        [1.0000, 0.5003],\n",
       "        [0.5782, 0.4674],\n",
       "        [1.0000, 0.4632],\n",
       "        [1.0000, 0.5148],\n",
       "        [1.0000, 0.5014],\n",
       "        [1.0000, 0.3764],\n",
       "        [0.9109, 0.4694],\n",
       "        [0.8509, 0.5520],\n",
       "        [1.0000, 0.4972],\n",
       "        [1.0000, 0.5001],\n",
       "        [1.0000, 0.4768],\n",
       "        [1.0000, 0.4482],\n",
       "        [0.6655, 0.5431],\n",
       "        [1.0000, 0.4939],\n",
       "        [1.0000, 0.5629],\n",
       "        [1.0000, 0.5161],\n",
       "        [1.0000, 0.4705],\n",
       "        [1.0000, 0.5018],\n",
       "        [1.0000, 0.4584],\n",
       "        [1.0000, 0.4986],\n",
       "        [1.0000, 0.4928],\n",
       "        [1.0000, 0.5000],\n",
       "        [1.0000, 0.5003],\n",
       "        [1.0000, 0.5041],\n",
       "        [1.0000, 0.4901],\n",
       "        [1.0000, 0.5044],\n",
       "        [0.7364, 0.3847],\n",
       "        [1.0000, 0.4614],\n",
       "        [1.0000, 0.5335],\n",
       "        [1.0000, 0.5979],\n",
       "        [1.0000, 0.5143],\n",
       "        [1.0000, 0.4892],\n",
       "        [1.0000, 0.4962],\n",
       "        [1.0000, 0.3751],\n",
       "        [0.3164, 0.5015],\n",
       "        [1.0000, 0.4897],\n",
       "        [1.0000, 0.4461],\n",
       "        [1.0000, 0.4811],\n",
       "        [0.7473, 0.5075],\n",
       "        [1.0000, 0.5000],\n",
       "        [1.0000, 0.4642],\n",
       "        [1.0000, 0.5033],\n",
       "        [1.0000, 0.4910],\n",
       "        [1.0000, 0.4941],\n",
       "        [1.0000, 0.4624],\n",
       "        [0.3709, 0.7322],\n",
       "        [0.6600, 0.3587],\n",
       "        [1.0000, 0.5067],\n",
       "        [0.7764, 0.3086],\n",
       "        [0.7418, 0.5074],\n",
       "        [1.0000, 0.4321],\n",
       "        [1.0000, 0.4805],\n",
       "        [1.0000, 0.5285],\n",
       "        [1.0000, 0.5458],\n",
       "        [1.0000, 0.3038],\n",
       "        [1.0000, 0.4942],\n",
       "        [1.0000, 0.5069],\n",
       "        [1.0000, 0.4682],\n",
       "        [1.0000, 0.5025],\n",
       "        [0.8291, 0.5003]], dtype=torch.float64)"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNNModel(nn.Module):\n",
    "    def __init__(self, num_lidar_features, num_non_lidar_features, num_actions, nframes=1):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.act_fea_cv1 = nn.Conv1d(\n",
    "            in_channels=nframes, out_channels=32, kernel_size=5, stride=2, padding=6, padding_mode='circular'\n",
    "        )\n",
    "        self.act_fea_cv2 = nn.Conv1d(\n",
    "            in_channels=32, out_channels=32, kernel_size=3, stride=2, padding=1\n",
    "        )\n",
    "\n",
    "        conv_output_size = (num_lidar_features - 5 + 2*6) // 2 + 1  # Output size after self.act_fea_cv1\n",
    "        conv_output_size = (conv_output_size - 3 + 2*1) // 2 + 1  # Output size after self.act_fea_cv2\n",
    "        conv_output_size *= 32  # Multiply by the number of output channels\n",
    "\n",
    "        # Calculate the output size of the CNN\n",
    "        self.fc1 = nn.Linear(conv_output_size, 64)\n",
    "        self.fc2 = nn.Linear(64 + num_non_lidar_features, 32)\n",
    "        self.fc3 = nn.Linear(32, num_actions)\n",
    "\n",
    "        torch.nn.init.xavier_uniform_(self.fc1.weight)\n",
    "        torch.nn.init.xavier_uniform_(self.fc2.weight)\n",
    "\n",
    "    def forward(self, lidar, non_lidar):\n",
    "        lidar = lidar.unsqueeze(1)  # Add channel dimension\n",
    "        feat = F.relu(self.act_fea_cv1(lidar))\n",
    "        feat = F.relu(self.act_fea_cv2(feat))\n",
    "        feat = feat.view(feat.shape[0], -1)\n",
    "        feat = F.relu(self.fc1(feat))\n",
    "        feat = torch.cat((feat, non_lidar), dim=-1)\n",
    "        feat = F.relu(self.fc2(feat))\n",
    "        feat = self.fc3(feat)\n",
    "        return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "num_lidar_features = len(train_dataset.lidar_cols)\n",
    "num_non_lidar_features = len(train_dataset.non_lidar_cols)\n",
    "num_actions = len(train_dataset.actions_cols)\n",
    "model = CNNModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "# Move the model and loss function to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(train_loader):\n",
    "\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device)\n",
    "        non_lidar = non_lidar.to(device)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(test_loader):\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device)\n",
    "        non_lidar = non_lidar.to(device)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "NUM_EPOCHS = 0\n",
    "\n",
    "# # random_val_loss = test_model(model, val_loader, loss_fn)\n",
    "# print(\"Random val loss:\", random_val_loss)\n",
    "sys.stdout.flush()\n",
    "\n",
    "cnn_train_losses = []\n",
    "cnn_val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    val_loss = test_model(model, val_loader, loss_fn)\n",
    "    cnn_train_losses.append(train_loss)\n",
    "    cnn_val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping due to no improvement after {} epochs.\".format(patience))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(cnn_train_losses, label='Train Loss')\n",
    "plt.plot(cnn_val_losses, label='Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'cnn_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # load file and check MSELoss\n",
    "# model = CNNModel(num_lidar_features, num_non_lidar_features, num_actions)\n",
    "# model.load_state_dict(torch.load('cnn_model.pth', map_location=torch.device('cpu')))\n",
    "# model.eval()\n",
    "# device = 'cpu'\n",
    "\n",
    "# # take world idx 0 as example\n",
    "# dataset = KULBarnDataset(df[df['world_idx'] == 0][df['timestep']>0], \"val\")\n",
    "# loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# final_val_loss = test_model(model, loader, loss_fn)\n",
    "# print(\"Final val loss:\", final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim,\n",
    "            num_heads,\n",
    "            dropout=0.0,\n",
    "            bias=False,\n",
    "            encoder_decoder_attention=False,\n",
    "            causal=False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.dropout = dropout\n",
    "        self.head_dim = input_dim // num_heads\n",
    "        self.encoder_decoder_attention = encoder_decoder_attention\n",
    "        self.causal = causal\n",
    "        self.k_proj = nn.Linear(input_dim, input_dim, bias=bias)\n",
    "        self.v_proj = nn.Linear(input_dim, input_dim, bias=bias)\n",
    "        self.q_proj = nn.Linear(input_dim, input_dim, bias=bias)\n",
    "        self.out_proj = nn.Linear(input_dim, input_dim, bias=bias)\n",
    "\n",
    "    def transpose_for_scores(self, x):\n",
    "        new_x_shape = x.size()[:-1] + (self.num_heads, self.head_dim,)\n",
    "        x = x.view(*new_x_shape)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "    def multi_head_scaled_dot_product(self,\n",
    "                                      query: torch.Tensor,\n",
    "                                      key: torch.Tensor,\n",
    "                                      value: torch.Tensor,\n",
    "                                      attention_mask: torch.BoolTensor):\n",
    "        attn_weights = torch.matmul(query, key.transpose(-1, -2) / math.sqrt(self.input_dim))\n",
    "        if attention_mask is not None:\n",
    "            if self.causal:\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(0).unsqueeze(1), float(\"-inf\"))\n",
    "            else:\n",
    "                attn_weights = attn_weights.masked_fill(attention_mask.unsqueeze(1).unsqueeze(2), float(\"-inf\"))\n",
    "        attn_weights = F.softmax(attn_weights, dim=-1)\n",
    "        attn_probs = F.dropout(attn_weights, p=self.dropout, training=self.training)\n",
    "        attn_output = torch.matmul(attn_probs, value)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous()\n",
    "        concat_attn_output_shape = attn_output.size()[:-2] + (self.input_dim,)\n",
    "        attn_output = attn_output.view(*concat_attn_output_shape)\n",
    "        attn_output = self.out_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            query: torch.Tensor,\n",
    "            key: torch.Tensor,\n",
    "            attention_mask: torch.BoolTensor):\n",
    "        q = self.q_proj(query)\n",
    "        if self.encoder_decoder_attention:\n",
    "            k = self.k_proj(key)\n",
    "            v = self.v_proj(key)\n",
    "        else:\n",
    "            k = self.k_proj(query)\n",
    "            v = self.v_proj(query)\n",
    "        q = self.transpose_for_scores(q)\n",
    "        k = self.transpose_for_scores(k)\n",
    "        v = self.transpose_for_scores(v)\n",
    "\n",
    "        attn_output, attn_weights = self.multi_head_scaled_dot_product(q, k, v, attention_mask)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim: int, d_ff: int, dropout: float = 0.1):\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "\n",
    "        self.activation = nn.ReLU()\n",
    "        self.w_1 = nn.Linear(input_dim, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, input_dim)\n",
    "        self.dropout = dropout\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.activation(self.w_1(x))\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = self.w_2(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "\n",
    "        return x + residual\n",
    "\n",
    "\n",
    "class EmbeddingLidar(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.len_lidar = 720\n",
    "        self.num_patch = config.num_patch\n",
    "        self.dim_patch = self.len_lidar // self.num_patch\n",
    "        self.model_dim = config.model_dim\n",
    "        self.dropout = config.dropout\n",
    "        self.pos_embed = nn.Parameter(torch.randn(self.num_patch, self.model_dim))\n",
    "\n",
    "        self.linear = nn.Linear(self.dim_patch, self.model_dim)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        x = inputs.view([-1, self.num_patch, self.dim_patch])\n",
    "        x = self.linear(x)\n",
    "        x = x + self.pos_embed\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        return x\n",
    "\n",
    "\n",
    "class EncoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = config.input_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.self_attn = MultiHeadAttention(\n",
    "            input_dim=self.input_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout)\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.input_dim)\n",
    "        self.dropout = config.dropout\n",
    "        self.activation_fn = nn.ReLU()\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.input_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.input_dim)\n",
    "\n",
    "    def forward(self, x, encoder_padding_mask):\n",
    "        residual = x\n",
    "        x, attn_weights = self.self_attn(query=x, key=x, attention_mask=encoder_padding_mask)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.self_attn_layer_norm(x)\n",
    "        x = self.PositionWiseFeedForward(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "        return x, attn_weights\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dropout = config.dropout\n",
    "\n",
    "        self.embedding = EmbeddingLidar(config)\n",
    "\n",
    "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
    "\n",
    "    def forward(self, inputs, attention_mask=None):\n",
    "        x = self.embedding(inputs)\n",
    "        self_attn_scores = []\n",
    "        for encoder_layer in self.layers:\n",
    "            x, attn = encoder_layer(x, attention_mask)\n",
    "            self_attn_scores.append(attn.detach())\n",
    "\n",
    "        return x, self_attn_scores\n",
    "\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.input_dim = config.input_dim\n",
    "        self.ffn_dim = config.ffn_dim\n",
    "        self.dropout = config.dropout\n",
    "        self.self_attn_layer_norm = nn.LayerNorm(self.input_dim)\n",
    "        self.encoder_attn = MultiHeadAttention(\n",
    "            input_dim=self.input_dim,\n",
    "            num_heads=config.attention_heads,\n",
    "            dropout=config.attention_dropout,\n",
    "            encoder_decoder_attention=True,\n",
    "        )\n",
    "        self.encoder_attn_layer_norm = nn.LayerNorm(self.input_dim)\n",
    "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.input_dim, self.ffn_dim, config.dropout)\n",
    "        self.final_layer_norm = nn.LayerNorm(self.input_dim)\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            x,\n",
    "            encoder_hidden_states,\n",
    "            encoder_attention_mask=None,\n",
    "    ):\n",
    "        residual = x\n",
    "        x, cross_attn_weights = self.encoder_attn(\n",
    "            query=x,\n",
    "            key=encoder_hidden_states,\n",
    "            attention_mask=encoder_attention_mask,\n",
    "        )\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        x = residual + x\n",
    "        x = self.encoder_attn_layer_norm(x)\n",
    "        x = self.PositionWiseFeedForward(x)\n",
    "        x = self.final_layer_norm(x)\n",
    "\n",
    "        return (\n",
    "            x,\n",
    "            cross_attn_weights,\n",
    "        )\n",
    "\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.dropout = config.dropout\n",
    "        self.model_dim = config.model_dim\n",
    "        self.linear = nn.Linear(1, self.model_dim)\n",
    "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            inputs,\n",
    "            encoder_hidden_states,\n",
    "    ):\n",
    "        x = inputs\n",
    "        x = self.linear(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        cross_attention_scores = []\n",
    "        for idx, decoder_layer in enumerate(self.layers):\n",
    "            x, layer_cross_attn = decoder_layer(\n",
    "                x,\n",
    "                encoder_hidden_states,\n",
    "            )\n",
    "            cross_attention_scores.append(layer_cross_attn.detach())\n",
    "        return x, cross_attention_scores\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.model_dim = config.model_dim\n",
    "        self.encoder = Encoder(config)\n",
    "        self.decoder = Decoder(config)\n",
    "\n",
    "        self.prediction_head = nn.Linear(self.model_dim * 2, 2)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        for name, param in self.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                if 'weight' in name:\n",
    "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
    "                else:\n",
    "                    nn.init.constant_(param.data, 0)\n",
    "\n",
    "    def forward(self, src, trg):\n",
    "        encoder_output, encoder_attention_scores = self.encoder(\n",
    "            inputs=src\n",
    "        )\n",
    "        decoder_output, decoder_attention_scores = self.decoder(\n",
    "            trg,\n",
    "            encoder_output\n",
    "        )\n",
    "        decoder_output = decoder_output.view(-1, self.model_dim * 2)\n",
    "        decoder_output = self.prediction_head(decoder_output)\n",
    "        \n",
    "        return decoder_output, encoder_attention_scores, decoder_attention_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, loss_fn, optimizer):\n",
    "    model.train()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(train_loader):\n",
    "        lidar = lidar.to(device).unsqueeze(-1)\n",
    "        non_lidar = non_lidar.to(device).unsqueeze(-1)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        actions_pred, _, _ = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(test_loader):\n",
    "        lidar = lidar.to(device).unsqueeze(-1)\n",
    "        non_lidar = non_lidar.to(device).unsqueeze(-1)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        actions_pred, _, _ = model(lidar.float(), non_lidar.float())\n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import easydict\n",
    "\n",
    "# Initialize the model\n",
    "num_lidar_features = len(train_dataset.lidar_cols)\n",
    "num_non_lidar_features = len(train_dataset.non_lidar_cols)\n",
    "num_actions = len(train_dataset.actions_cols)\n",
    "\n",
    "config_dict = easydict.EasyDict({\n",
    "    \"input_dim\": 32,\n",
    "    \"num_patch\": 36,\n",
    "    \"model_dim\": 32,\n",
    "    \"ffn_dim\": 256,\n",
    "    \"attention_heads\": 4,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.2,\n",
    "    \"encoder_layers\": 3,\n",
    "    \"decoder_layers\": 3,\n",
    "    \"device\": device,\n",
    "})\n",
    "\n",
    "model = Transformer(config_dict)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Move the model and loss function to the GPU if available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "loss_fn = loss_fn.to(device)\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 310/310 [00:02<00:00, 151.59it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random val loss: 0.5685312477090666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2194/2194 [00:26<00:00, 81.53it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 211.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100 | Train Loss: 0.5571266180456174 | Val Loss: 0.5544575936130939\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 79.14it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 207.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100 | Train Loss: 0.546223169092493 | Val Loss: 0.5436342889022443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 78.38it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 184.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100 | Train Loss: 0.5345336212797825 | Val Loss: 0.5309382210816106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 78.46it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 205.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100 | Train Loss: 0.5210909432949755 | Val Loss: 0.5167696050097865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 80.68it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 206.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100 | Train Loss: 0.5062515145964044 | Val Loss: 0.5012798699159776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 79.72it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 206.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100 | Train Loss: 0.4901173519974094 | Val Loss: 0.4844622971550111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 79.88it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 198.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100 | Train Loss: 0.4726863709382828 | Val Loss: 0.4663704291707085\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:42<00:00, 51.78it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 99.81it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100 | Train Loss: 0.454058450139862 | Val Loss: 0.4471225512844901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:31<00:00, 69.64it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 187.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100 | Train Loss: 0.434348737177462 | Val Loss: 0.4268349029484295\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:32<00:00, 66.88it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 167.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100 | Train Loss: 0.4136613420632718 | Val Loss: 0.40564589112276034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:36<00:00, 60.81it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 198.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100 | Train Loss: 0.3921694955232476 | Val Loss: 0.38369539406511094\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:37<00:00, 58.06it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 96.41it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100 | Train Loss: 0.3699874845464988 | Val Loss: 0.3611260969251875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:38<00:00, 56.87it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 209.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100 | Train Loss: 0.3472682121893223 | Val Loss: 0.338102028572992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:30<00:00, 71.83it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 175.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100 | Train Loss: 0.324191743363656 | Val Loss: 0.3147743183758951\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:32<00:00, 66.75it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 202.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100 | Train Loss: 0.3008927734553434 | Val Loss: 0.2913153313565999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:31<00:00, 69.61it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 135.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100 | Train Loss: 0.277548781875436 | Val Loss: 0.2678957426139424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:28<00:00, 75.91it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 193.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100 | Train Loss: 0.25435581329934903 | Val Loss: 0.244716787131338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:38<00:00, 57.19it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 141.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100 | Train Loss: 0.23149409199451032 | Val Loss: 0.22196072386127086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:42<00:00, 52.21it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 126.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100 | Train Loss: 0.2091575792651885 | Val Loss: 0.19981017119641747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 54.09it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 132.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100 | Train Loss: 0.1875393302090731 | Val Loss: 0.1784811276777257\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 53.46it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 132.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100 | Train Loss: 0.1668322328325283 | Val Loss: 0.15815233665428335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 53.90it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 108.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100 | Train Loss: 0.1472331070211277 | Val Loss: 0.13902694925125086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 54.48it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 130.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100 | Train Loss: 0.12895436273546468 | Val Loss: 0.12132086419470368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 54.15it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 139.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100 | Train Loss: 0.11216669325583505 | Val Loss: 0.10517288178145405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 52.63it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 126.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100 | Train Loss: 0.09700846808912435 | Val Loss: 0.0906854853875214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 53.27it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 120.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100 | Train Loss: 0.08344462177730629 | Val Loss: 0.07766112935759367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 53.16it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 127.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100 | Train Loss: 0.07111285967495948 | Val Loss: 0.06566082717279993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 54.33it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 126.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100 | Train Loss: 0.05979400722746319 | Val Loss: 0.05473839721432136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 52.51it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 137.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100 | Train Loss: 0.04964736583341276 | Val Loss: 0.04504664226795637\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 52.54it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 122.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 | Train Loss: 0.04055516290119293 | Val Loss: 0.03585734518410097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 52.94it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 131.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100 | Train Loss: 0.031902045543825995 | Val Loss: 0.027665094885554527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 54.82it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 144.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100 | Train Loss: 0.024535633582849336 | Val Loss: 0.02083181269737261\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 53.64it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 123.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100 | Train Loss: 0.01869777583881343 | Val Loss: 0.015866973090376105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:43<00:00, 50.96it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 102.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 | Train Loss: 0.014461719430346338 | Val Loss: 0.012376576863349446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:43<00:00, 49.93it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 130.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 | Train Loss: 0.011668525422255126 | Val Loss: 0.010183095237270238\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:38<00:00, 57.45it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 118.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/100 | Train Loss: 0.01010360364117061 | Val Loss: 0.00913344681781206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:42<00:00, 51.53it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 117.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/100 | Train Loss: 0.00936526662800393 | Val Loss: 0.008471554200326453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:41<00:00, 53.18it/s]\n",
      "100%|██████████| 310/310 [00:03<00:00, 97.20it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 | Train Loss: 0.008947824101866229 | Val Loss: 0.008370586726889616\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:40<00:00, 53.65it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 190.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 | Train Loss: 0.008605364640795183 | Val Loss: 0.008191030008874218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:28<00:00, 76.11it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 187.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/100 | Train Loss: 0.008271505111760953 | Val Loss: 0.007956614533383888\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:28<00:00, 77.38it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 184.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/100 | Train Loss: 0.007905516472926115 | Val Loss: 0.007715382820676877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:29<00:00, 75.44it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 191.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 | Train Loss: 0.007429931224028791 | Val Loss: 0.007098943401374967\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:28<00:00, 77.61it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 192.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 | Train Loss: 0.006918433705315506 | Val Loss: 0.00715696982582402\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:27<00:00, 79.81it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 193.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 | Train Loss: 0.006492847493166696 | Val Loss: 0.005917869533679491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:26<00:00, 82.39it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 206.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/100 | Train Loss: 0.0061877408361936 | Val Loss: 0.006083793143549091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:28<00:00, 77.64it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 165.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/100 | Train Loss: 0.005963563540903299 | Val Loss: 0.0057814523354365165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:36<00:00, 60.83it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 147.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 | Train Loss: 0.005788303262275221 | Val Loss: 0.006041371849395575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:35<00:00, 62.57it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 188.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/100 | Train Loss: 0.005652192352501355 | Val Loss: 0.005862352345074244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:34<00:00, 64.18it/s]\n",
      "100%|██████████| 310/310 [00:01<00:00, 193.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/100 | Train Loss: 0.0055573889345767735 | Val Loss: 0.00562046864075866\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:31<00:00, 68.89it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 108.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 | Train Loss: 0.005478517849322191 | Val Loss: 0.0053847676450870146\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:45<00:00, 48.14it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 108.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/100 | Train Loss: 0.005390919240678889 | Val Loss: 0.0054418300911141075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:45<00:00, 47.97it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 106.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 | Train Loss: 0.0053594393523854825 | Val Loss: 0.0058605946217275846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|██████████| 2194/2194 [00:43<00:00, 50.46it/s]\n",
      "100%|██████████| 310/310 [00:02<00:00, 123.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 | Train Loss: 0.005302370164764972 | Val Loss: 0.005795082320459656\n",
      "Early stopping due to no improvement after 3 epochs.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "NUM_EPOCHS = 100\n",
    "\n",
    "random_val_loss = test_model(model, val_loader, loss_fn)\n",
    "print(\"Random val loss:\", random_val_loss)\n",
    "\n",
    "transformer_train_losses = []\n",
    "transformer_val_losses = []\n",
    "best_val_loss = float('inf')\n",
    "patience = 3\n",
    "no_improve_epochs = 0\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    train_loss = train_model(model, train_loader, loss_fn, optimizer)\n",
    "    val_loss = test_model(model, val_loader, loss_fn)\n",
    "    transformer_train_losses.append(train_loss)\n",
    "    transformer_val_losses.append(val_loss)\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS} | Train Loss: {train_loss} | Val Loss: {val_loss}\")\n",
    "    sys.stdout.flush()\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        no_improve_epochs = 0\n",
    "    else:\n",
    "        no_improve_epochs += 1\n",
    "        if no_improve_epochs >= patience:\n",
    "            print(\"Early stopping due to no improvement after {} epochs.\".format(patience))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(transformer_train_losses, label='Transformer Train Loss')  \n",
    "plt.plot(transformer_val_losses, label='Transformer Val Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "torch.save(model.state_dict(), 'transformer_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, test_loader, loss_fn):\n",
    "    model.eval()\n",
    "\n",
    "    losses = []\n",
    "    for lidar, non_lidar, actions in tqdm(test_loader):\n",
    "        # Move the data to the device that is used\n",
    "        lidar = lidar.to(device).unsqueeze(-1)\n",
    "        non_lidar = non_lidar.to(device).unsqueeze(-1)\n",
    "        actions = actions.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        actions_pred, _, _ = model(lidar.float(), non_lidar.float())        \n",
    "        loss = loss_fn(actions_pred, actions.float())\n",
    "\n",
    "        print(non_lidar)\n",
    "        print(actions_pred)\n",
    "        print(actions)\n",
    "        print(loss)\n",
    "\n",
    "        # Save the loss\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # return the average loss for this epoch\n",
    "    return sum(losses)/len(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/11 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 36%|███▋      | 4/11 [00:00<00:00, 35.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.9949],\n",
      "         [0.5713]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5713]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5713]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5715]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5719]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5721]],\n",
      "\n",
      "        [[0.9947],\n",
      "         [0.5724]],\n",
      "\n",
      "        [[0.9947],\n",
      "         [0.5728]],\n",
      "\n",
      "        [[0.9946],\n",
      "         [0.5732]],\n",
      "\n",
      "        [[0.9946],\n",
      "         [0.5736]],\n",
      "\n",
      "        [[0.9945],\n",
      "         [0.5740]],\n",
      "\n",
      "        [[0.9944],\n",
      "         [0.5745]],\n",
      "\n",
      "        [[0.9943],\n",
      "         [0.5750]],\n",
      "\n",
      "        [[0.9943],\n",
      "         [0.5755]],\n",
      "\n",
      "        [[0.9950],\n",
      "         [0.5707]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5712]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5717]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5722]],\n",
      "\n",
      "        [[0.9947],\n",
      "         [0.5728]],\n",
      "\n",
      "        [[0.9946],\n",
      "         [0.5734]],\n",
      "\n",
      "        [[0.9945],\n",
      "         [0.5740]],\n",
      "\n",
      "        [[0.9944],\n",
      "         [0.5747]],\n",
      "\n",
      "        [[0.9951],\n",
      "         [0.5699]],\n",
      "\n",
      "        [[0.9950],\n",
      "         [0.5704]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5710]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5716]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5722]],\n",
      "\n",
      "        [[0.9947],\n",
      "         [0.5729]],\n",
      "\n",
      "        [[0.9946],\n",
      "         [0.5736]],\n",
      "\n",
      "        [[0.9952],\n",
      "         [0.5689]],\n",
      "\n",
      "        [[0.9951],\n",
      "         [0.5696]],\n",
      "\n",
      "        [[0.9950],\n",
      "         [0.5702]],\n",
      "\n",
      "        [[0.9949],\n",
      "         [0.5710]],\n",
      "\n",
      "        [[0.9948],\n",
      "         [0.5719]],\n",
      "\n",
      "        [[0.9955],\n",
      "         [0.5673]],\n",
      "\n",
      "        [[0.9954],\n",
      "         [0.5679]],\n",
      "\n",
      "        [[0.9953],\n",
      "         [0.5687]],\n",
      "\n",
      "        [[0.9951],\n",
      "         [0.5696]],\n",
      "\n",
      "        [[0.9950],\n",
      "         [0.5705]],\n",
      "\n",
      "        [[0.9956],\n",
      "         [0.5659]],\n",
      "\n",
      "        [[0.9955],\n",
      "         [0.5668]],\n",
      "\n",
      "        [[0.9954],\n",
      "         [0.5676]],\n",
      "\n",
      "        [[0.9953],\n",
      "         [0.5685]],\n",
      "\n",
      "        [[0.9959],\n",
      "         [0.5642]],\n",
      "\n",
      "        [[0.9957],\n",
      "         [0.5655]],\n",
      "\n",
      "        [[0.9955],\n",
      "         [0.5666]],\n",
      "\n",
      "        [[0.9954],\n",
      "         [0.5680]],\n",
      "\n",
      "        [[0.9959],\n",
      "         [0.5639]],\n",
      "\n",
      "        [[0.9958],\n",
      "         [0.5650]],\n",
      "\n",
      "        [[0.9956],\n",
      "         [0.5663]],\n",
      "\n",
      "        [[0.9954],\n",
      "         [0.5676]],\n",
      "\n",
      "        [[0.9960],\n",
      "         [0.5635]],\n",
      "\n",
      "        [[0.9958],\n",
      "         [0.5647]],\n",
      "\n",
      "        [[0.9957],\n",
      "         [0.5658]],\n",
      "\n",
      "        [[0.9962],\n",
      "         [0.5617]],\n",
      "\n",
      "        [[0.9960],\n",
      "         [0.5628]],\n",
      "\n",
      "        [[0.9959],\n",
      "         [0.5639]],\n",
      "\n",
      "        [[0.9964],\n",
      "         [0.5600]],\n",
      "\n",
      "        [[0.9963],\n",
      "         [0.5611]],\n",
      "\n",
      "        [[0.9961],\n",
      "         [0.5623]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.5584]],\n",
      "\n",
      "        [[0.9964],\n",
      "         [0.5596]],\n",
      "\n",
      "        [[0.9963],\n",
      "         [0.5609]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.5572]]], dtype=torch.float64)\n",
      "tensor([[0.4298, 0.4423],\n",
      "        [0.4300, 0.4424],\n",
      "        [0.4299, 0.4423],\n",
      "        [0.4294, 0.4421],\n",
      "        [0.4296, 0.4422],\n",
      "        [0.4290, 0.4419],\n",
      "        [0.4296, 0.4422],\n",
      "        [0.4302, 0.4425],\n",
      "        [0.4301, 0.4424],\n",
      "        [0.4308, 0.4428],\n",
      "        [0.4341, 0.4444],\n",
      "        [0.4348, 0.4448],\n",
      "        [0.4386, 0.4467],\n",
      "        [0.4403, 0.4476],\n",
      "        [0.4427, 0.4488],\n",
      "        [0.4456, 0.4503],\n",
      "        [0.4484, 0.4516],\n",
      "        [0.4521, 0.4535],\n",
      "        [0.4553, 0.4550],\n",
      "        [0.4586, 0.4567],\n",
      "        [0.4634, 0.4590],\n",
      "        [0.4574, 0.4560],\n",
      "        [0.4608, 0.4577],\n",
      "        [0.4694, 0.4619],\n",
      "        [0.4692, 0.4618],\n",
      "        [0.4683, 0.4613],\n",
      "        [0.4719, 0.4630],\n",
      "        [0.4791, 0.4664],\n",
      "        [0.4900, 0.4715],\n",
      "        [0.5031, 0.4774],\n",
      "        [0.5041, 0.4779],\n",
      "        [0.5014, 0.4767],\n",
      "        [0.5152, 0.4828],\n",
      "        [0.5085, 0.4799],\n",
      "        [0.5142, 0.4823],\n",
      "        [0.5183, 0.4841],\n",
      "        [0.5181, 0.4841],\n",
      "        [0.5100, 0.4805],\n",
      "        [0.5065, 0.4789],\n",
      "        [0.5196, 0.4846],\n",
      "        [0.5106, 0.4808],\n",
      "        [0.4986, 0.4754],\n",
      "        [0.4972, 0.4748],\n",
      "        [0.5000, 0.4760],\n",
      "        [0.5000, 0.4760],\n",
      "        [0.4902, 0.4716],\n",
      "        [0.4832, 0.4683],\n",
      "        [0.4742, 0.4641],\n",
      "        [0.4625, 0.4586],\n",
      "        [0.4588, 0.4568],\n",
      "        [0.4692, 0.4618],\n",
      "        [0.4787, 0.4662],\n",
      "        [0.4850, 0.4692],\n",
      "        [0.4820, 0.4678],\n",
      "        [0.4883, 0.4706],\n",
      "        [0.4911, 0.4719],\n",
      "        [0.4963, 0.4743],\n",
      "        [0.5264, 0.4874],\n",
      "        [0.5556, 0.4994],\n",
      "        [0.5471, 0.4960],\n",
      "        [0.5573, 0.5000],\n",
      "        [0.5735, 0.5063],\n",
      "        [0.5839, 0.5102],\n",
      "        [0.5889, 0.5119]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.3327, 0.5021],\n",
      "        [0.3327, 0.5023],\n",
      "        [0.3327, 0.5023],\n",
      "        [0.3382, 0.5025],\n",
      "        [0.3436, 0.5028],\n",
      "        [0.3491, 0.5030],\n",
      "        [0.3545, 0.5032],\n",
      "        [0.3600, 0.5034],\n",
      "        [0.3655, 0.5036],\n",
      "        [0.3709, 0.5038],\n",
      "        [0.3764, 0.5040],\n",
      "        [0.3818, 0.5042],\n",
      "        [0.3873, 0.5045],\n",
      "        [0.3927, 0.5047],\n",
      "        [0.3982, 0.5049],\n",
      "        [0.4036, 0.5051],\n",
      "        [0.4091, 0.5053],\n",
      "        [0.4145, 0.5055],\n",
      "        [0.4200, 0.5058],\n",
      "        [0.4255, 0.5060],\n",
      "        [0.4309, 0.5066],\n",
      "        [0.4364, 0.5068],\n",
      "        [0.4418, 0.5065],\n",
      "        [0.4473, 0.5067],\n",
      "        [0.4527, 0.5069],\n",
      "        [0.4582, 0.5071],\n",
      "        [0.4636, 0.5073],\n",
      "        [0.4691, 0.5065],\n",
      "        [0.4745, 0.5067],\n",
      "        [0.4800, 0.5068],\n",
      "        [0.4855, 0.5070],\n",
      "        [0.4909, 0.5072],\n",
      "        [0.4964, 0.5074],\n",
      "        [0.5018, 0.5075],\n",
      "        [0.5073, 0.5077],\n",
      "        [0.5127, 0.5073],\n",
      "        [0.5182, 0.5075],\n",
      "        [0.5236, 0.5075],\n",
      "        [0.5291, 0.5077],\n",
      "        [0.5345, 0.5078],\n",
      "        [0.5400, 0.5084],\n",
      "        [0.5455, 0.5085],\n",
      "        [0.5509, 0.5076],\n",
      "        [0.5564, 0.5078],\n",
      "        [0.5618, 0.5079],\n",
      "        [0.5673, 0.5075],\n",
      "        [0.5727, 0.5076],\n",
      "        [0.5782, 0.5080],\n",
      "        [0.5836, 0.5081],\n",
      "        [0.5891, 0.5082],\n",
      "        [0.5945, 0.5084],\n",
      "        [0.6000, 0.5085],\n",
      "        [0.6055, 0.5082],\n",
      "        [0.6109, 0.5083],\n",
      "        [0.6164, 0.5084],\n",
      "        [0.6218, 0.5085],\n",
      "        [0.6273, 0.5086],\n",
      "        [0.6327, 0.5088],\n",
      "        [0.6382, 0.5089],\n",
      "        [0.6436, 0.5090],\n",
      "        [0.6491, 0.5082],\n",
      "        [0.6545, 0.5083],\n",
      "        [0.6600, 0.5072],\n",
      "        [0.6655, 0.5073]], dtype=torch.float64)\n",
      "tensor(0.0035, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9966],\n",
      "         [0.5586]],\n",
      "\n",
      "        [[0.9964],\n",
      "         [0.5601]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.5565]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.5578]],\n",
      "\n",
      "        [[0.9965],\n",
      "         [0.5593]],\n",
      "\n",
      "        [[0.9969],\n",
      "         [0.5556]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.5570]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.5584]],\n",
      "\n",
      "        [[0.9970],\n",
      "         [0.5548]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.5561]],\n",
      "\n",
      "        [[0.9972],\n",
      "         [0.5525]],\n",
      "\n",
      "        [[0.9971],\n",
      "         [0.5537]],\n",
      "\n",
      "        [[0.9970],\n",
      "         [0.5551]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.5516]],\n",
      "\n",
      "        [[0.9972],\n",
      "         [0.5529]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.5495]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.5512]],\n",
      "\n",
      "        [[0.9972],\n",
      "         [0.5528]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.5496]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.5512]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.5480]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.5494]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.5463]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.5478]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.5449]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.5462]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.5479]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.5448]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.5465]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.5436]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.5452]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.5425]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.5440]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.5412]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.5428]],\n",
      "\n",
      "        [[0.9984],\n",
      "         [0.5401]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.5414]],\n",
      "\n",
      "        [[0.9985],\n",
      "         [0.5388]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.5362]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.5373]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.5349]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.5362]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.5340]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.5352]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.5330]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.5343]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.5319]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.5332]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.5310]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.5288]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.5301]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.5280]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.5294]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.5273]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.5287]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.5270]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.5268]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5253]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.5268]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5252]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5237]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5249]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5233]]], dtype=torch.float64)\n",
      "tensor([[0.5909, 0.5127],\n",
      "        [0.5884, 0.5118],\n",
      "        [0.6081, 0.5186],\n",
      "        [0.6062, 0.5180],\n",
      "        [0.5824, 0.5096],\n",
      "        [0.5699, 0.5047],\n",
      "        [0.5315, 0.4895],\n",
      "        [0.5358, 0.4912],\n",
      "        [0.5682, 0.5040],\n",
      "        [0.6000, 0.5157],\n",
      "        [0.5886, 0.5115],\n",
      "        [0.5990, 0.5153],\n",
      "        [0.6045, 0.5173],\n",
      "        [0.6207, 0.5226],\n",
      "        [0.6120, 0.5198],\n",
      "        [0.5936, 0.5132],\n",
      "        [0.6178, 0.5216],\n",
      "        [0.6144, 0.5206],\n",
      "        [0.6113, 0.5193],\n",
      "        [0.6189, 0.5220],\n",
      "        [0.6203, 0.5222],\n",
      "        [0.6511, 0.5320],\n",
      "        [0.6127, 0.5195],\n",
      "        [0.6236, 0.5232],\n",
      "        [0.6341, 0.5263],\n",
      "        [0.6562, 0.5332],\n",
      "        [0.6692, 0.5371],\n",
      "        [0.6984, 0.5445],\n",
      "        [0.7015, 0.5456],\n",
      "        [0.6624, 0.5348],\n",
      "        [0.6939, 0.5435],\n",
      "        [0.7038, 0.5456],\n",
      "        [0.7444, 0.5549],\n",
      "        [0.7465, 0.5549],\n",
      "        [0.7161, 0.5486],\n",
      "        [0.6685, 0.5361],\n",
      "        [0.6939, 0.5430],\n",
      "        [0.7520, 0.5555],\n",
      "        [0.7505, 0.5549],\n",
      "        [0.7642, 0.5577],\n",
      "        [0.7678, 0.5579],\n",
      "        [0.7738, 0.5592],\n",
      "        [0.7698, 0.5581],\n",
      "        [0.8198, 0.5654],\n",
      "        [0.8457, 0.5674],\n",
      "        [0.8522, 0.5681],\n",
      "        [0.8535, 0.5676],\n",
      "        [0.8295, 0.5658],\n",
      "        [0.8621, 0.5678],\n",
      "        [0.8737, 0.5680],\n",
      "        [0.8750, 0.5684],\n",
      "        [0.8696, 0.5677],\n",
      "        [0.8567, 0.5673],\n",
      "        [0.8693, 0.5675],\n",
      "        [0.8580, 0.5671],\n",
      "        [0.8571, 0.5666],\n",
      "        [0.8591, 0.5664],\n",
      "        [0.8334, 0.5647],\n",
      "        [0.8445, 0.5654],\n",
      "        [0.8855, 0.5678],\n",
      "        [0.8881, 0.5674],\n",
      "        [0.8882, 0.5671],\n",
      "        [0.8672, 0.5668],\n",
      "        [0.8707, 0.5665]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[0.6709, 0.5074],\n",
      "        [0.6764, 0.5071],\n",
      "        [0.6818, 0.5072],\n",
      "        [0.6873, 0.5074],\n",
      "        [0.6927, 0.5075],\n",
      "        [0.6982, 0.5076],\n",
      "        [0.7036, 0.5071],\n",
      "        [0.7091, 0.5072],\n",
      "        [0.7145, 0.5078],\n",
      "        [0.7200, 0.5079],\n",
      "        [0.7255, 0.5080],\n",
      "        [0.7309, 0.5072],\n",
      "        [0.7364, 0.5073],\n",
      "        [0.7418, 0.5074],\n",
      "        [0.7473, 0.5075],\n",
      "        [0.7527, 0.5076],\n",
      "        [0.7582, 0.5056],\n",
      "        [0.7636, 0.5056],\n",
      "        [0.7691, 0.5060],\n",
      "        [0.7745, 0.5061],\n",
      "        [0.7800, 0.5062],\n",
      "        [0.7855, 0.5059],\n",
      "        [0.7909, 0.5059],\n",
      "        [0.7964, 0.5060],\n",
      "        [0.8018, 0.5061],\n",
      "        [0.8073, 0.5061],\n",
      "        [0.8127, 0.5054],\n",
      "        [0.8182, 0.5054],\n",
      "        [0.8236, 0.5055],\n",
      "        [0.8291, 0.5060],\n",
      "        [0.8345, 0.5061],\n",
      "        [0.8400, 0.5057],\n",
      "        [0.8455, 0.5057],\n",
      "        [0.8509, 0.5051],\n",
      "        [0.8564, 0.5051],\n",
      "        [0.8618, 0.5052],\n",
      "        [0.8673, 0.5056],\n",
      "        [0.8727, 0.5057],\n",
      "        [0.8782, 0.5051],\n",
      "        [0.8836, 0.5051],\n",
      "        [0.8891, 0.5052],\n",
      "        [0.8945, 0.5040],\n",
      "        [0.9000, 0.5040],\n",
      "        [0.9055, 0.5042],\n",
      "        [0.9109, 0.5043],\n",
      "        [0.9164, 0.5043],\n",
      "        [0.9218, 0.5038],\n",
      "        [0.9273, 0.5038],\n",
      "        [0.9327, 0.5032],\n",
      "        [0.9382, 0.5032],\n",
      "        [0.9436, 0.5033],\n",
      "        [0.9491, 0.4932],\n",
      "        [0.9545, 0.4932],\n",
      "        [0.9600, 0.4921],\n",
      "        [0.9655, 0.4920],\n",
      "        [0.9709, 0.4919],\n",
      "        [0.9764, 0.4918],\n",
      "        [0.9818, 0.4917],\n",
      "        [0.9873, 0.4916],\n",
      "        [0.9927, 0.4921],\n",
      "        [0.9982, 0.4920],\n",
      "        [1.0000, 0.4911],\n",
      "        [1.0000, 0.4911],\n",
      "        [1.0000, 0.4911]], dtype=torch.float64)\n",
      "tensor(0.0093, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9995],\n",
      "         [0.5220]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5231]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5215]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5213]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5199]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5191]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5203]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5193]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5206]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5197]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5188]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5202]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5194]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5186]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5199]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5191]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5183]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5193]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5188]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5193]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5188]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5203]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.5154]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5111]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5072]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5203]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5160]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5225]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.5259]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5221]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.5359]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.5317]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.5220]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5182]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.5147]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5182]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.5146]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.5163]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5198]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.5287]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.5324]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.5413]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.5370]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.5408]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.5499]],\n",
      "\n",
      "        [[0.9971],\n",
      "         [0.5541]],\n",
      "\n",
      "        [[0.8470],\n",
      "         [0.8600]],\n",
      "\n",
      "        [[0.8413],\n",
      "         [0.8654]],\n",
      "\n",
      "        [[0.8322],\n",
      "         [0.8737]],\n",
      "\n",
      "        [[0.8466],\n",
      "         [0.8603]],\n",
      "\n",
      "        [[0.8400],\n",
      "         [0.8666]],\n",
      "\n",
      "        [[0.9857],\n",
      "         [0.3814]],\n",
      "\n",
      "        [[0.9824],\n",
      "         [0.3684]],\n",
      "\n",
      "        [[0.9773],\n",
      "         [0.3510]],\n",
      "\n",
      "        [[0.9838],\n",
      "         [0.3739]],\n",
      "\n",
      "        [[0.9802],\n",
      "         [0.3607]],\n",
      "\n",
      "        [[0.9846],\n",
      "         [0.3768]],\n",
      "\n",
      "        [[0.9838],\n",
      "         [0.3737]],\n",
      "\n",
      "        [[0.9816],\n",
      "         [0.3657]],\n",
      "\n",
      "        [[0.9876],\n",
      "         [0.3894]]], dtype=torch.float64)\n",
      "tensor([[0.8592, 0.5656],\n",
      "        [0.8720, 0.5664],\n",
      "        [0.8612, 0.5656],\n",
      "        [0.8734, 0.5659],\n",
      "        [0.8826, 0.5664],\n",
      "        [0.8818, 0.5660],\n",
      "        [0.8716, 0.5655],\n",
      "        [0.8710, 0.5659],\n",
      "        [0.8815, 0.5659],\n",
      "        [0.8961, 0.5664],\n",
      "        [0.8866, 0.5662],\n",
      "        [0.8797, 0.5657],\n",
      "        [0.8802, 0.5660],\n",
      "        [0.8802, 0.5660],\n",
      "        [0.8390, 0.5635],\n",
      "        [0.8371, 0.5632],\n",
      "        [0.8446, 0.5641],\n",
      "        [0.8468, 0.5640],\n",
      "        [0.8402, 0.5633],\n",
      "        [0.8439, 0.5639],\n",
      "        [0.8485, 0.5641],\n",
      "        [0.8570, 0.5650],\n",
      "        [0.8507, 0.5643],\n",
      "        [0.8513, 0.5642],\n",
      "        [0.8560, 0.5648],\n",
      "        [0.8753, 0.5646],\n",
      "        [0.8700, 0.5633],\n",
      "        [0.8815, 0.5628],\n",
      "        [0.8904, 0.5662],\n",
      "        [0.9182, 0.5645],\n",
      "        [0.9040, 0.5666],\n",
      "        [0.8856, 0.5675],\n",
      "        [0.8987, 0.5666],\n",
      "        [0.8711, 0.5695],\n",
      "        [0.9205, 0.5681],\n",
      "        [0.9401, 0.5639],\n",
      "        [0.9652, 0.5580],\n",
      "        [0.9309, 0.5631],\n",
      "        [0.9503, 0.5614],\n",
      "        [0.9468, 0.5612],\n",
      "        [0.9007, 0.5661],\n",
      "        [0.8724, 0.5649],\n",
      "        [0.8645, 0.5655],\n",
      "        [0.9322, 0.5665],\n",
      "        [0.9009, 0.5691],\n",
      "        [0.9102, 0.5709],\n",
      "        [0.8890, 0.5701],\n",
      "        [0.8729, 0.5707],\n",
      "        [0.8640, 0.5721],\n",
      "        [0.8836, 0.5736],\n",
      "        [0.8283, 0.5844],\n",
      "        [0.8568, 0.5885],\n",
      "        [0.8322, 0.5850],\n",
      "        [0.8363, 0.5859],\n",
      "        [0.9069, 0.5918],\n",
      "        [0.8411, 0.5127],\n",
      "        [0.7930, 0.4930],\n",
      "        [0.6973, 0.4507],\n",
      "        [0.8879, 0.4937],\n",
      "        [0.7768, 0.4736],\n",
      "        [0.8688, 0.5028],\n",
      "        [0.8461, 0.4998],\n",
      "        [0.7618, 0.4861],\n",
      "        [0.8879, 0.5221]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.4907],\n",
      "        [1.0000, 0.4907],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4882],\n",
      "        [1.0000, 0.4882],\n",
      "        [1.0000, 0.4882],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4888],\n",
      "        [1.0000, 0.4888],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4886],\n",
      "        [1.0000, 0.4892],\n",
      "        [1.0000, 0.4884],\n",
      "        [1.0000, 0.4432],\n",
      "        [1.0000, 0.4432],\n",
      "        [1.0000, 0.4432],\n",
      "        [1.0000, 0.4443],\n",
      "        [1.0000, 0.4443],\n",
      "        [1.0000, 0.4459],\n",
      "        [1.0000, 0.4459],\n",
      "        [1.0000, 0.4459],\n",
      "        [1.0000, 0.4531],\n",
      "        [1.0000, 0.4531],\n",
      "        [1.0000, 0.4363],\n",
      "        [1.0000, 0.4363],\n",
      "        [1.0000, 0.4363],\n",
      "        [1.0000, 0.4373],\n",
      "        [1.0000, 0.4373],\n",
      "        [1.0000, 0.4381],\n",
      "        [1.0000, 0.4381],\n",
      "        [1.0000, 0.4381],\n",
      "        [1.0000, 0.4451],\n",
      "        [1.0000, 0.4451],\n",
      "        [1.0000, 0.4508],\n",
      "        [1.0000, 0.4508],\n",
      "        [1.0000, 0.4508],\n",
      "        [1.0000, 0.4561],\n",
      "        [1.0000, 0.4561],\n",
      "        [1.0000, 0.9195],\n",
      "        [1.0000, 0.9152],\n",
      "        [1.0000, 0.9152],\n",
      "        [1.0000, 0.9209],\n",
      "        [1.0000, 0.9209],\n",
      "        [1.0000, 0.3440],\n",
      "        [1.0000, 0.3440],\n",
      "        [1.0000, 0.3440],\n",
      "        [1.0000, 0.3213],\n",
      "        [1.0000, 0.3213],\n",
      "        [1.0000, 0.3050],\n",
      "        [1.0000, 0.3050],\n",
      "        [1.0000, 0.3050],\n",
      "        [1.0000, 0.2972],\n",
      "        [1.0000, 0.2972],\n",
      "        [1.0000, 0.2969]], dtype=torch.float64)\n",
      "tensor(0.0209, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9862],\n",
      "         [0.3832]],\n",
      "\n",
      "        [[0.9850],\n",
      "         [0.3783]],\n",
      "\n",
      "        [[0.9855],\n",
      "         [0.3805]],\n",
      "\n",
      "        [[0.9849],\n",
      "         [0.3781]],\n",
      "\n",
      "        [[0.9907],\n",
      "         [0.4039]],\n",
      "\n",
      "        [[0.9900],\n",
      "         [0.4003]],\n",
      "\n",
      "        [[0.9934],\n",
      "         [0.4191]],\n",
      "\n",
      "        [[0.9940],\n",
      "         [0.4225]],\n",
      "\n",
      "        [[0.9934],\n",
      "         [0.4193]],\n",
      "\n",
      "        [[0.9891],\n",
      "         [0.3964]],\n",
      "\n",
      "        [[0.9896],\n",
      "         [0.3983]],\n",
      "\n",
      "        [[0.9892],\n",
      "         [0.3965]],\n",
      "\n",
      "        [[0.9894],\n",
      "         [0.3977]],\n",
      "\n",
      "        [[0.9893],\n",
      "         [0.3971]],\n",
      "\n",
      "        [[0.9932],\n",
      "         [0.4181]],\n",
      "\n",
      "        [[0.9935],\n",
      "         [0.4199]],\n",
      "\n",
      "        [[0.9894],\n",
      "         [0.3978]],\n",
      "\n",
      "        [[0.9887],\n",
      "         [0.3945]],\n",
      "\n",
      "        [[0.9881],\n",
      "         [0.3917]],\n",
      "\n",
      "        [[0.9932],\n",
      "         [0.4178]],\n",
      "\n",
      "        [[0.9928],\n",
      "         [0.4156]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.4507]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.4478]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.4505]],\n",
      "\n",
      "        [[0.9902],\n",
      "         [0.4017]],\n",
      "\n",
      "        [[0.9902],\n",
      "         [0.4016]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.4432]],\n",
      "\n",
      "        [[0.9965],\n",
      "         [0.4409]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4635]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4647]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4882]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4915]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4885]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.4479]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.4486]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4729]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4692]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4680]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4920]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4917]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.4438]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.4425]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.4429]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4744]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4728]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5057]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5022]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4990]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4907]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4893]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4852]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4846]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4844]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4767]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4774]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5045]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5038]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5064]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.5251]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.5279]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4974]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4980]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4997]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5194]]], dtype=torch.float64)\n",
      "tensor([[0.7967, 0.5157],\n",
      "        [0.7547, 0.5060],\n",
      "        [0.7531, 0.5080],\n",
      "        [0.7914, 0.5079],\n",
      "        [0.8926, 0.5321],\n",
      "        [0.9261, 0.5261],\n",
      "        [0.9703, 0.5252],\n",
      "        [0.9705, 0.5271],\n",
      "        [0.9754, 0.5219],\n",
      "        [0.9606, 0.5067],\n",
      "        [0.9697, 0.5009],\n",
      "        [0.9691, 0.4977],\n",
      "        [0.9648, 0.5063],\n",
      "        [0.9674, 0.5016],\n",
      "        [0.9875, 0.5081],\n",
      "        [0.9882, 0.5088],\n",
      "        [0.9779, 0.4756],\n",
      "        [0.9732, 0.4810],\n",
      "        [0.9694, 0.4785],\n",
      "        [0.9912, 0.4884],\n",
      "        [0.9901, 0.4916],\n",
      "        [1.0025, 0.5038],\n",
      "        [1.0017, 0.4938],\n",
      "        [1.0027, 0.4986],\n",
      "        [0.9777, 0.4944],\n",
      "        [0.9784, 0.4906],\n",
      "        [0.9949, 0.5187],\n",
      "        [0.9983, 0.5101],\n",
      "        [1.0035, 0.5142],\n",
      "        [1.0051, 0.5063],\n",
      "        [1.0073, 0.4978],\n",
      "        [1.0082, 0.5017],\n",
      "        [1.0083, 0.5046],\n",
      "        [1.0018, 0.4955],\n",
      "        [1.0016, 0.4925],\n",
      "        [1.0062, 0.5001],\n",
      "        [1.0058, 0.5028],\n",
      "        [1.0057, 0.5046],\n",
      "        [1.0084, 0.5036],\n",
      "        [1.0086, 0.5082],\n",
      "        [1.0010, 0.4965],\n",
      "        [0.9995, 0.5084],\n",
      "        [1.0000, 0.5073],\n",
      "        [1.0060, 0.5126],\n",
      "        [1.0063, 0.5087],\n",
      "        [1.0095, 0.5164],\n",
      "        [1.0091, 0.5155],\n",
      "        [1.0070, 0.5230],\n",
      "        [1.0081, 0.5141],\n",
      "        [1.0078, 0.5149],\n",
      "        [1.0041, 0.5239],\n",
      "        [1.0068, 0.5163],\n",
      "        [1.0069, 0.5158],\n",
      "        [1.0065, 0.5117],\n",
      "        [1.0063, 0.5133],\n",
      "        [1.0096, 0.5096],\n",
      "        [1.0096, 0.5098],\n",
      "        [1.0097, 0.5133],\n",
      "        [1.0110, 0.5144],\n",
      "        [1.0112, 0.5159],\n",
      "        [1.0081, 0.5179],\n",
      "        [1.0090, 0.5124],\n",
      "        [1.0091, 0.5083],\n",
      "        [1.0106, 0.5110]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.2969],\n",
      "        [1.0000, 0.2969],\n",
      "        [1.0000, 0.3147],\n",
      "        [1.0000, 0.3147],\n",
      "        [1.0000, 0.3280],\n",
      "        [1.0000, 0.3280],\n",
      "        [1.0000, 0.3280],\n",
      "        [1.0000, 0.3340],\n",
      "        [1.0000, 0.3340],\n",
      "        [1.0000, 0.3453],\n",
      "        [1.0000, 0.3453],\n",
      "        [1.0000, 0.3453],\n",
      "        [1.0000, 0.3594],\n",
      "        [1.0000, 0.3594],\n",
      "        [1.0000, 0.3451],\n",
      "        [1.0000, 0.3451],\n",
      "        [1.0000, 0.3451],\n",
      "        [1.0000, 0.3624],\n",
      "        [1.0000, 0.3624],\n",
      "        [1.0000, 0.3743],\n",
      "        [1.0000, 0.3743],\n",
      "        [1.0000, 0.3743],\n",
      "        [1.0000, 0.3680],\n",
      "        [1.0000, 0.3687],\n",
      "        [1.0000, 0.3895],\n",
      "        [1.0000, 0.3905],\n",
      "        [1.0000, 0.4054],\n",
      "        [1.0000, 0.4054],\n",
      "        [1.0000, 0.4128],\n",
      "        [1.0000, 0.4128],\n",
      "        [1.0000, 0.4137],\n",
      "        [1.0000, 0.4082],\n",
      "        [1.0000, 0.4082],\n",
      "        [1.0000, 0.4206],\n",
      "        [1.0000, 0.4206],\n",
      "        [1.0000, 0.4206],\n",
      "        [1.0000, 0.4282],\n",
      "        [1.0000, 0.4282],\n",
      "        [1.0000, 0.4175],\n",
      "        [1.0000, 0.4155],\n",
      "        [1.0000, 0.4155],\n",
      "        [1.0000, 0.4183],\n",
      "        [1.0000, 0.4204],\n",
      "        [1.0000, 0.4295],\n",
      "        [1.0000, 0.4295],\n",
      "        [1.0000, 0.4295],\n",
      "        [1.0000, 0.4287],\n",
      "        [1.0000, 0.4287],\n",
      "        [1.0000, 0.4289],\n",
      "        [1.0000, 0.4289],\n",
      "        [1.0000, 0.4289],\n",
      "        [1.0000, 0.4284],\n",
      "        [1.0000, 0.4284],\n",
      "        [1.0000, 0.4364],\n",
      "        [1.0000, 0.4367],\n",
      "        [1.0000, 0.4367],\n",
      "        [1.0000, 0.4496],\n",
      "        [1.0000, 0.4462],\n",
      "        [1.0000, 0.4460],\n",
      "        [1.0000, 0.4460],\n",
      "        [1.0000, 0.4489],\n",
      "        [1.0000, 0.4517],\n",
      "        [1.0000, 0.4522],\n",
      "        [1.0000, 0.4522]], dtype=torch.float64)\n",
      "tensor(0.0091, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9997],\n",
      "         [0.5175]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.5158]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4740]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4734]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4746]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4747]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4748]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4985]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4972]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4963]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4940]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4935]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4935]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4956]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4955]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4894]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4901]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5069]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5083]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5061]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4888]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4889]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5004]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4990]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5000]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4967]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4960]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4967]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4942]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4950]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4923]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4921]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4741]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4795]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4873]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4872]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5025]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4689]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.4667]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4728]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4864]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4848]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4735]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4713]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4686]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.4665]],\n",
      "\n",
      "        [[0.9925],\n",
      "         [0.4140]],\n",
      "\n",
      "        [[0.9916],\n",
      "         [0.4089]],\n",
      "\n",
      "        [[0.9892],\n",
      "         [0.3965]],\n",
      "\n",
      "        [[0.9879],\n",
      "         [0.3905]],\n",
      "\n",
      "        [[0.9865],\n",
      "         [0.3846]],\n",
      "\n",
      "        [[0.9874],\n",
      "         [0.3883]],\n",
      "\n",
      "        [[0.9785],\n",
      "         [0.3548]],\n",
      "\n",
      "        [[0.9893],\n",
      "         [0.3970]],\n",
      "\n",
      "        [[0.9882],\n",
      "         [0.3918]],\n",
      "\n",
      "        [[0.9900],\n",
      "         [0.4006]],\n",
      "\n",
      "        [[0.9900],\n",
      "         [0.4006]],\n",
      "\n",
      "        [[0.9912],\n",
      "         [0.4067]],\n",
      "\n",
      "        [[0.9944],\n",
      "         [0.4254]],\n",
      "\n",
      "        [[0.9929],\n",
      "         [0.4158]],\n",
      "\n",
      "        [[0.9920],\n",
      "         [0.4112]],\n",
      "\n",
      "        [[0.9919],\n",
      "         [0.4104]],\n",
      "\n",
      "        [[0.9927],\n",
      "         [0.4149]],\n",
      "\n",
      "        [[0.9971],\n",
      "         [0.4462]]], dtype=torch.float64)\n",
      "tensor([[1.0104, 0.5098],\n",
      "        [1.0104, 0.5105],\n",
      "        [1.0065, 0.5082],\n",
      "        [1.0064, 0.5087],\n",
      "        [1.0057, 0.5137],\n",
      "        [1.0052, 0.5155],\n",
      "        [1.0065, 0.5075],\n",
      "        [1.0091, 0.5087],\n",
      "        [1.0089, 0.5069],\n",
      "        [1.0087, 0.5048],\n",
      "        [1.0088, 0.5068],\n",
      "        [1.0085, 0.5041],\n",
      "        [1.0086, 0.5040],\n",
      "        [1.0090, 0.5084],\n",
      "        [1.0090, 0.5074],\n",
      "        [1.0084, 0.5092],\n",
      "        [1.0084, 0.5093],\n",
      "        [1.0098, 0.5142],\n",
      "        [1.0100, 0.5131],\n",
      "        [1.0097, 0.5146],\n",
      "        [1.0077, 0.5146],\n",
      "        [1.0078, 0.5143],\n",
      "        [1.0088, 0.5165],\n",
      "        [1.0073, 0.5220],\n",
      "        [1.0059, 0.5260],\n",
      "        [0.9979, 0.5364],\n",
      "        [0.9959, 0.5381],\n",
      "        [0.9973, 0.5370],\n",
      "        [0.9861, 0.5443],\n",
      "        [0.9829, 0.5461],\n",
      "        [0.9866, 0.5434],\n",
      "        [0.9885, 0.5423],\n",
      "        [0.9838, 0.5400],\n",
      "        [0.9863, 0.5401],\n",
      "        [0.9716, 0.5487],\n",
      "        [0.9698, 0.5493],\n",
      "        [0.9778, 0.5502],\n",
      "        [0.9740, 0.5431],\n",
      "        [0.9795, 0.5401],\n",
      "        [0.9777, 0.5426],\n",
      "        [0.9709, 0.5487],\n",
      "        [0.9655, 0.5499],\n",
      "        [0.9774, 0.5429],\n",
      "        [0.9830, 0.5397],\n",
      "        [0.9807, 0.5401],\n",
      "        [0.9842, 0.5376],\n",
      "        [0.9663, 0.5233],\n",
      "        [0.9679, 0.5176],\n",
      "        [0.9517, 0.5127],\n",
      "        [0.9365, 0.5104],\n",
      "        [0.9242, 0.5034],\n",
      "        [0.9329, 0.5070],\n",
      "        [0.8146, 0.4464],\n",
      "        [0.9608, 0.5061],\n",
      "        [0.9540, 0.5014],\n",
      "        [0.9655, 0.5075],\n",
      "        [0.9655, 0.5075],\n",
      "        [0.9729, 0.5097],\n",
      "        [0.9825, 0.5205],\n",
      "        [0.9767, 0.5173],\n",
      "        [0.9815, 0.5063],\n",
      "        [0.9807, 0.5062],\n",
      "        [0.9819, 0.5108],\n",
      "        [0.9989, 0.5135]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.4489],\n",
      "        [1.0000, 0.4361],\n",
      "        [1.0000, 0.4361],\n",
      "        [1.0000, 0.4408],\n",
      "        [1.0000, 0.4404],\n",
      "        [1.0000, 0.4477],\n",
      "        [1.0000, 0.4478],\n",
      "        [1.0000, 0.4478],\n",
      "        [1.0000, 0.4487],\n",
      "        [1.0000, 0.4488],\n",
      "        [1.0000, 0.4483],\n",
      "        [1.0000, 0.4484],\n",
      "        [1.0000, 0.4487],\n",
      "        [1.0000, 0.4646],\n",
      "        [1.0000, 0.4654],\n",
      "        [1.0000, 0.4676],\n",
      "        [1.0000, 0.4676],\n",
      "        [1.0000, 0.4676],\n",
      "        [1.0000, 0.4716],\n",
      "        [1.0000, 0.4727],\n",
      "        [1.0000, 0.4771],\n",
      "        [1.0000, 0.4773],\n",
      "        [1.0000, 0.4754],\n",
      "        [1.0000, 0.4739],\n",
      "        [1.0000, 0.4739],\n",
      "        [1.0000, 0.4758],\n",
      "        [1.0000, 0.4752],\n",
      "        [1.0000, 0.4754],\n",
      "        [1.0000, 0.4739],\n",
      "        [1.0000, 0.4739],\n",
      "        [1.0000, 0.4747],\n",
      "        [1.0000, 0.4674],\n",
      "        [1.0000, 0.4676],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5097],\n",
      "        [1.0000, 0.5156],\n",
      "        [1.0000, 0.5177],\n",
      "        [1.0000, 0.5169],\n",
      "        [1.0000, 0.5201],\n",
      "        [1.0000, 0.5202],\n",
      "        [1.0000, 0.5296],\n",
      "        [1.0000, 0.5280],\n",
      "        [1.0000, 0.5308],\n",
      "        [1.0000, 0.5212],\n",
      "        [1.0000, 0.5212],\n",
      "        [1.0000, 0.5251],\n",
      "        [1.0000, 0.5223],\n",
      "        [1.0000, 0.5236],\n",
      "        [1.0000, 0.5411],\n",
      "        [1.0000, 0.5448],\n",
      "        [1.0000, 0.5053],\n",
      "        [1.0000, 0.5183],\n",
      "        [1.0000, 0.5064],\n",
      "        [1.0000, 0.5325],\n",
      "        [1.0000, 0.5333],\n",
      "        [1.0000, 0.5137],\n",
      "        [1.0000, 0.5137],\n",
      "        [1.0000, 0.5366],\n",
      "        [1.0000, 0.5232],\n",
      "        [1.0000, 0.5107],\n",
      "        [1.0000, 0.5111],\n",
      "        [1.0000, 0.5559],\n",
      "        [1.0000, 0.7217],\n",
      "        [1.0000, 0.7309]], dtype=torch.float64)\n",
      "tensor(0.0023, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9962],\n",
      "         [0.4384]],\n",
      "\n",
      "        [[0.9958],\n",
      "         [0.4350]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.4501]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4648]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4643]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.4506]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.4506]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.4550]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.4415]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.4429]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.4524]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4686]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4778]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4694]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4732]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4651]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4765]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4884]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4979]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5027]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4939]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4966]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4977]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4882]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4918]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4941]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4851]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4976]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5053]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.5487]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.5487]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.5571]],\n",
      "\n",
      "        [[0.9961],\n",
      "         [0.5620]],\n",
      "\n",
      "        [[0.9969],\n",
      "         [0.5558]],\n",
      "\n",
      "        [[0.9942],\n",
      "         [0.5756]],\n",
      "\n",
      "        [[0.9863],\n",
      "         [0.6162]],\n",
      "\n",
      "        [[0.9861],\n",
      "         [0.6171]],\n",
      "\n",
      "        [[0.9866],\n",
      "         [0.6150]],\n",
      "\n",
      "        [[0.9899],\n",
      "         [0.5998]],\n",
      "\n",
      "        [[0.9942],\n",
      "         [0.5761]],\n",
      "\n",
      "        [[0.9898],\n",
      "         [0.6003]],\n",
      "\n",
      "        [[0.9904],\n",
      "         [0.5976]],\n",
      "\n",
      "        [[0.9878],\n",
      "         [0.6099]],\n",
      "\n",
      "        [[0.9862],\n",
      "         [0.6166]],\n",
      "\n",
      "        [[0.9862],\n",
      "         [0.6167]],\n",
      "\n",
      "        [[0.9801],\n",
      "         [0.6398]],\n",
      "\n",
      "        [[0.9761],\n",
      "         [0.6526]],\n",
      "\n",
      "        [[0.9736],\n",
      "         [0.6603]],\n",
      "\n",
      "        [[0.9718],\n",
      "         [0.6654]],\n",
      "\n",
      "        [[0.9750],\n",
      "         [0.6561]],\n",
      "\n",
      "        [[0.9765],\n",
      "         [0.6515]],\n",
      "\n",
      "        [[0.9747],\n",
      "         [0.6569]],\n",
      "\n",
      "        [[0.9791],\n",
      "         [0.6432]],\n",
      "\n",
      "        [[0.9854],\n",
      "         [0.6201]],\n",
      "\n",
      "        [[0.9883],\n",
      "         [0.6074]],\n",
      "\n",
      "        [[0.9876],\n",
      "         [0.6106]],\n",
      "\n",
      "        [[0.9904],\n",
      "         [0.5977]],\n",
      "\n",
      "        [[0.9924],\n",
      "         [0.5867]],\n",
      "\n",
      "        [[0.9941],\n",
      "         [0.5765]],\n",
      "\n",
      "        [[0.9935],\n",
      "         [0.5802]],\n",
      "\n",
      "        [[0.9950],\n",
      "         [0.5707]],\n",
      "\n",
      "        [[0.9962],\n",
      "         [0.5614]],\n",
      "\n",
      "        [[0.9958],\n",
      "         [0.5646]],\n",
      "\n",
      "        [[0.9970],\n",
      "         [0.5550]]], dtype=torch.float64)\n",
      "tensor([[0.9932, 0.5183],\n",
      "        [0.9928, 0.5162],\n",
      "        [0.9912, 0.5268],\n",
      "        [0.9918, 0.5318],\n",
      "        [0.9853, 0.5363],\n",
      "        [0.9952, 0.5226],\n",
      "        [0.9697, 0.5393],\n",
      "        [0.9675, 0.5414],\n",
      "        [0.9859, 0.5276],\n",
      "        [0.9791, 0.5325],\n",
      "        [0.9796, 0.5357],\n",
      "        [0.9909, 0.5340],\n",
      "        [0.9760, 0.5449],\n",
      "        [0.9713, 0.5444],\n",
      "        [0.9577, 0.5492],\n",
      "        [0.9417, 0.5499],\n",
      "        [0.9082, 0.5552],\n",
      "        [0.9124, 0.5581],\n",
      "        [0.9099, 0.5606],\n",
      "        [0.9146, 0.5616],\n",
      "        [0.9098, 0.5595],\n",
      "        [0.8865, 0.5603],\n",
      "        [0.8869, 0.5606],\n",
      "        [0.8464, 0.5561],\n",
      "        [0.8632, 0.5581],\n",
      "        [0.8665, 0.5589],\n",
      "        [0.9015, 0.5572],\n",
      "        [0.8955, 0.5605],\n",
      "        [0.9162, 0.5621],\n",
      "        [0.9065, 0.5729],\n",
      "        [0.9065, 0.5729],\n",
      "        [0.8702, 0.5742],\n",
      "        [0.9272, 0.5744],\n",
      "        [0.9399, 0.5718],\n",
      "        [0.9362, 0.5758],\n",
      "        [0.9385, 0.5813],\n",
      "        [0.9565, 0.5781],\n",
      "        [0.9499, 0.5792],\n",
      "        [0.9292, 0.5802],\n",
      "        [0.9309, 0.5765],\n",
      "        [0.9098, 0.5815],\n",
      "        [0.9346, 0.5794],\n",
      "        [0.9174, 0.5824],\n",
      "        [0.9251, 0.5827],\n",
      "        [0.9430, 0.5807],\n",
      "        [0.9758, 0.5747],\n",
      "        [0.9765, 0.5755],\n",
      "        [0.9743, 0.5769],\n",
      "        [0.9803, 0.5749],\n",
      "        [0.9886, 0.5700],\n",
      "        [0.9901, 0.5686],\n",
      "        [0.9996, 0.5614],\n",
      "        [1.0059, 0.5531],\n",
      "        [1.0077, 0.5488],\n",
      "        [1.0098, 0.5440],\n",
      "        [1.0106, 0.5428],\n",
      "        [1.0119, 0.5378],\n",
      "        [1.0130, 0.5323],\n",
      "        [1.0134, 0.5281],\n",
      "        [1.0141, 0.5229],\n",
      "        [1.0138, 0.5208],\n",
      "        [1.0134, 0.5156],\n",
      "        [1.0129, 0.5110],\n",
      "        [1.0123, 0.5082]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.7858],\n",
      "        [1.0000, 0.7858],\n",
      "        [1.0000, 0.7808],\n",
      "        [1.0000, 0.7748],\n",
      "        [1.0000, 0.7748],\n",
      "        [1.0000, 0.7506],\n",
      "        [1.0000, 0.7506],\n",
      "        [1.0000, 0.7623],\n",
      "        [1.0000, 0.7578],\n",
      "        [1.0000, 0.7578],\n",
      "        [1.0000, 0.6947],\n",
      "        [1.0000, 0.6814],\n",
      "        [1.0000, 0.6814],\n",
      "        [1.0000, 0.6844],\n",
      "        [1.0000, 0.6519],\n",
      "        [1.0000, 0.6391],\n",
      "        [1.0000, 0.6391],\n",
      "        [1.0000, 0.7312],\n",
      "        [1.0000, 0.7312],\n",
      "        [1.0000, 0.7488],\n",
      "        [1.0000, 0.7354],\n",
      "        [1.0000, 0.7282],\n",
      "        [1.0000, 0.7155],\n",
      "        [1.0000, 0.7149],\n",
      "        [1.0000, 0.7219],\n",
      "        [1.0000, 0.7084],\n",
      "        [1.0000, 0.7040],\n",
      "        [1.0000, 0.7007],\n",
      "        [1.0000, 0.7197],\n",
      "        [1.0000, 0.7197],\n",
      "        [1.0000, 0.7197],\n",
      "        [1.0000, 0.7177],\n",
      "        [1.0000, 0.7159],\n",
      "        [1.0000, 0.7151],\n",
      "        [1.0000, 0.7112],\n",
      "        [1.0000, 0.6933],\n",
      "        [1.0000, 0.6799],\n",
      "        [1.0000, 0.6780],\n",
      "        [1.0000, 0.6780],\n",
      "        [1.0000, 0.6749],\n",
      "        [1.0000, 0.6749],\n",
      "        [1.0000, 0.6411],\n",
      "        [1.0000, 0.6411],\n",
      "        [1.0000, 0.6099],\n",
      "        [1.0000, 0.6493],\n",
      "        [1.0000, 0.6065],\n",
      "        [1.0000, 0.5773],\n",
      "        [1.0000, 0.5773],\n",
      "        [1.0000, 0.5574],\n",
      "        [1.0000, 0.5458],\n",
      "        [1.0000, 0.5326],\n",
      "        [1.0000, 0.5173],\n",
      "        [1.0000, 0.5173],\n",
      "        [1.0000, 0.5173],\n",
      "        [1.0000, 0.5127],\n",
      "        [1.0000, 0.5127],\n",
      "        [1.0000, 0.5106],\n",
      "        [1.0000, 0.5106],\n",
      "        [1.0000, 0.5106],\n",
      "        [1.0000, 0.5118],\n",
      "        [1.0000, 0.5118],\n",
      "        [1.0000, 0.5065],\n",
      "        [1.0000, 0.5065],\n",
      "        [1.0000, 0.5065]], dtype=torch.float64)\n",
      "tensor(0.0115, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9978],\n",
      "         [0.5472]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.5517]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.5439]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.5368]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.5364]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.5296]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5189]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.5200]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.5147]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5074]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5076]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4967]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4964]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4920]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4815]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4815]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4732]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4705]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4689]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4589]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4562]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4542]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.4519]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.4499]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.4423]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.4420]],\n",
      "\n",
      "        [[0.9957],\n",
      "         [0.4343]],\n",
      "\n",
      "        [[0.9956],\n",
      "         [0.4341]],\n",
      "\n",
      "        [[0.9957],\n",
      "         [0.4345]],\n",
      "\n",
      "        [[0.9945],\n",
      "         [0.4263]],\n",
      "\n",
      "        [[0.9945],\n",
      "         [0.4262]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4946]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4939]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4912]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4827]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4821]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4736]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4721]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4700]],\n",
      "\n",
      "        [[0.9985],\n",
      "         [0.4618]],\n",
      "\n",
      "        [[0.9984],\n",
      "         [0.4601]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4584]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.4573]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4560]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.4484]],\n",
      "\n",
      "        [[0.9971],\n",
      "         [0.4467]],\n",
      "\n",
      "        [[0.9964],\n",
      "         [0.4398]],\n",
      "\n",
      "        [[0.9964],\n",
      "         [0.4400]],\n",
      "\n",
      "        [[0.9961],\n",
      "         [0.4379]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5018]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4991]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4929]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4901]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4881]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4817]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4796]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4776]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4766]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4752]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.4671]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4658]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4584]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4569]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4564]]], dtype=torch.float64)\n",
      "tensor([[1.0108, 0.5023],\n",
      "        [1.0110, 0.5028],\n",
      "        [1.0108, 0.5021],\n",
      "        [1.0097, 0.4988],\n",
      "        [1.0101, 0.5002],\n",
      "        [1.0090, 0.4968],\n",
      "        [1.0089, 0.4973],\n",
      "        [1.0090, 0.4977],\n",
      "        [1.0085, 0.4966],\n",
      "        [1.0075, 0.4939],\n",
      "        [1.0088, 0.4987],\n",
      "        [1.0078, 0.4965],\n",
      "        [1.0078, 0.4968],\n",
      "        [1.0080, 0.4982],\n",
      "        [1.0063, 0.4941],\n",
      "        [1.0069, 0.4970],\n",
      "        [1.0064, 0.4988],\n",
      "        [1.0059, 0.4978],\n",
      "        [1.0058, 0.4986],\n",
      "        [1.0036, 0.4931],\n",
      "        [1.0039, 0.4995],\n",
      "        [1.0036, 0.5012],\n",
      "        [1.0030, 0.5048],\n",
      "        [1.0022, 0.5067],\n",
      "        [0.9991, 0.5100],\n",
      "        [0.9963, 0.5163],\n",
      "        [0.9912, 0.5182],\n",
      "        [0.9896, 0.5201],\n",
      "        [0.9887, 0.5216],\n",
      "        [0.9813, 0.5228],\n",
      "        [0.9781, 0.5252],\n",
      "        [0.9906, 0.5425],\n",
      "        [0.9870, 0.5445],\n",
      "        [0.9773, 0.5483],\n",
      "        [0.9670, 0.5495],\n",
      "        [0.9683, 0.5490],\n",
      "        [0.9585, 0.5492],\n",
      "        [0.9762, 0.5437],\n",
      "        [0.9603, 0.5479],\n",
      "        [0.9621, 0.5453],\n",
      "        [0.9496, 0.5475],\n",
      "        [0.9441, 0.5479],\n",
      "        [0.9232, 0.5496],\n",
      "        [0.9279, 0.5489],\n",
      "        [0.9150, 0.5474],\n",
      "        [0.9151, 0.5471],\n",
      "        [0.9194, 0.5452],\n",
      "        [0.9267, 0.5447],\n",
      "        [0.9410, 0.5426],\n",
      "        [0.9639, 0.5551],\n",
      "        [0.9617, 0.5549],\n",
      "        [0.9526, 0.5552],\n",
      "        [0.9527, 0.5546],\n",
      "        [0.9436, 0.5555],\n",
      "        [0.9365, 0.5546],\n",
      "        [0.9281, 0.5547],\n",
      "        [0.9276, 0.5543],\n",
      "        [0.9626, 0.5490],\n",
      "        [0.9662, 0.5477],\n",
      "        [0.9584, 0.5476],\n",
      "        [0.9572, 0.5475],\n",
      "        [0.9528, 0.5466],\n",
      "        [0.9470, 0.5471],\n",
      "        [0.9455, 0.5472]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.5043],\n",
      "        [1.0000, 0.5043],\n",
      "        [1.0000, 0.5022],\n",
      "        [1.0000, 0.5022],\n",
      "        [1.0000, 0.5022],\n",
      "        [1.0000, 0.4983],\n",
      "        [1.0000, 0.4983],\n",
      "        [1.0000, 0.4931],\n",
      "        [1.0000, 0.4931],\n",
      "        [1.0000, 0.4931],\n",
      "        [1.0000, 0.4862],\n",
      "        [1.0000, 0.4862],\n",
      "        [1.0000, 0.4827],\n",
      "        [1.0000, 0.4827],\n",
      "        [1.0000, 0.4827],\n",
      "        [1.0000, 0.4745],\n",
      "        [1.0000, 0.4745],\n",
      "        [1.0000, 0.4728],\n",
      "        [1.0000, 0.4728],\n",
      "        [1.0000, 0.4728],\n",
      "        [1.0000, 0.4718],\n",
      "        [1.0000, 0.4718],\n",
      "        [1.0000, 0.4685],\n",
      "        [1.0000, 0.4685],\n",
      "        [1.0000, 0.4685],\n",
      "        [1.0000, 0.4655],\n",
      "        [1.0000, 0.4655],\n",
      "        [1.0000, 0.4656],\n",
      "        [1.0000, 0.4656],\n",
      "        [1.0000, 0.4656],\n",
      "        [1.0000, 0.4954],\n",
      "        [1.0000, 0.4954],\n",
      "        [1.0000, 0.4937],\n",
      "        [1.0000, 0.4937],\n",
      "        [1.0000, 0.4937],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4891],\n",
      "        [1.0000, 0.4865],\n",
      "        [1.0000, 0.4865],\n",
      "        [1.0000, 0.4865],\n",
      "        [1.0000, 0.4858],\n",
      "        [1.0000, 0.4858],\n",
      "        [1.0000, 0.4804],\n",
      "        [1.0000, 0.4804],\n",
      "        [1.0000, 0.4804],\n",
      "        [1.0000, 0.4778],\n",
      "        [1.0000, 0.4778],\n",
      "        [1.0000, 0.4978],\n",
      "        [1.0000, 0.4978],\n",
      "        [1.0000, 0.4978],\n",
      "        [1.0000, 0.4959],\n",
      "        [1.0000, 0.4959],\n",
      "        [1.0000, 0.4933],\n",
      "        [1.0000, 0.4933],\n",
      "        [1.0000, 0.4933],\n",
      "        [1.0000, 0.4934],\n",
      "        [1.0000, 0.4934],\n",
      "        [1.0000, 0.4912],\n",
      "        [1.0000, 0.4912],\n",
      "        [1.0000, 0.4912],\n",
      "        [1.0000, 0.4866],\n",
      "        [1.0000, 0.4866],\n",
      "        [1.0000, 0.4849],\n",
      "        [1.0000, 0.4849]], dtype=torch.float64)\n",
      "tensor(0.0019, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9974],\n",
      "         [0.4495]],\n",
      "\n",
      "        [[0.9973],\n",
      "         [0.4478]],\n",
      "\n",
      "        [[0.9965],\n",
      "         [0.4410]],\n",
      "\n",
      "        [[0.9962],\n",
      "         [0.4388]],\n",
      "\n",
      "        [[0.9962],\n",
      "         [0.4387]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5074]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5048]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4979]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4957]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4953]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4868]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4849]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4779]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4762]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4748]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4737]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4727]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4641]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4634]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4631]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4544]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4544]],\n",
      "\n",
      "        [[0.9972],\n",
      "         [0.4475]],\n",
      "\n",
      "        [[0.9970],\n",
      "         [0.4454]],\n",
      "\n",
      "        [[0.9970],\n",
      "         [0.4453]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5057]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5054]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5029]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5009]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5007]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4919]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4902]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4823]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4808]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4797]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4715]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4705]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4690]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4685]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4682]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4588]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4585]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.4488]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.4487]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.4488]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5098]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5071]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4983]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4980]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4958]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4872]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4861]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4842]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4829]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4819]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4732]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4720]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4636]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4630]],\n",
      "\n",
      "        [[0.9985],\n",
      "         [0.4613]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.4534]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.4531]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.4433]],\n",
      "\n",
      "        [[0.9968],\n",
      "         [0.4431]]], dtype=torch.float64)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 11/11 [00:00<00:00, 39.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.9414, 0.5459],\n",
      "        [0.9352, 0.5461],\n",
      "        [0.9316, 0.5446],\n",
      "        [0.9272, 0.5444],\n",
      "        [0.9303, 0.5441],\n",
      "        [0.9290, 0.5617],\n",
      "        [0.9599, 0.5567],\n",
      "        [0.9620, 0.5546],\n",
      "        [0.9587, 0.5548],\n",
      "        [0.9541, 0.5556],\n",
      "        [0.9546, 0.5534],\n",
      "        [0.9509, 0.5535],\n",
      "        [0.9511, 0.5517],\n",
      "        [0.9528, 0.5510],\n",
      "        [0.9580, 0.5497],\n",
      "        [0.9577, 0.5494],\n",
      "        [0.9539, 0.5499],\n",
      "        [0.9485, 0.5486],\n",
      "        [0.9791, 0.5400],\n",
      "        [0.9843, 0.5374],\n",
      "        [0.9801, 0.5369],\n",
      "        [0.9800, 0.5369],\n",
      "        [0.9769, 0.5361],\n",
      "        [0.9810, 0.5333],\n",
      "        [0.9791, 0.5342],\n",
      "        [0.9875, 0.5470],\n",
      "        [0.9893, 0.5459],\n",
      "        [0.9905, 0.5445],\n",
      "        [0.9881, 0.5454],\n",
      "        [0.9993, 0.5370],\n",
      "        [0.9993, 0.5345],\n",
      "        [1.0005, 0.5325],\n",
      "        [0.9993, 0.5313],\n",
      "        [0.9990, 0.5313],\n",
      "        [0.9959, 0.5340],\n",
      "        [0.9967, 0.5306],\n",
      "        [0.9995, 0.5271],\n",
      "        [0.9992, 0.5268],\n",
      "        [0.9990, 0.5269],\n",
      "        [0.9983, 0.5277],\n",
      "        [0.9971, 0.5254],\n",
      "        [1.0008, 0.5197],\n",
      "        [0.9991, 0.5172],\n",
      "        [0.9988, 0.5178],\n",
      "        [0.9993, 0.5167],\n",
      "        [1.0080, 0.5262],\n",
      "        [1.0080, 0.5254],\n",
      "        [1.0068, 0.5249],\n",
      "        [1.0081, 0.5210],\n",
      "        [1.0079, 0.5209],\n",
      "        [1.0066, 0.5207],\n",
      "        [1.0052, 0.5236],\n",
      "        [1.0069, 0.5180],\n",
      "        [1.0072, 0.5159],\n",
      "        [1.0067, 0.5173],\n",
      "        [1.0053, 0.5168],\n",
      "        [1.0049, 0.5174],\n",
      "        [1.0045, 0.5130],\n",
      "        [1.0038, 0.5148],\n",
      "        [1.0036, 0.5142],\n",
      "        [1.0022, 0.5127],\n",
      "        [1.0023, 0.5121],\n",
      "        [1.0001, 0.5104],\n",
      "        [0.9998, 0.5111]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.4849],\n",
      "        [1.0000, 0.4830],\n",
      "        [1.0000, 0.4830],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5093],\n",
      "        [1.0000, 0.5093],\n",
      "        [1.0000, 0.5032],\n",
      "        [1.0000, 0.5032],\n",
      "        [1.0000, 0.5032],\n",
      "        [1.0000, 0.4971],\n",
      "        [1.0000, 0.4971],\n",
      "        [1.0000, 0.4965],\n",
      "        [1.0000, 0.4965],\n",
      "        [1.0000, 0.4965],\n",
      "        [1.0000, 0.4910],\n",
      "        [1.0000, 0.4910],\n",
      "        [1.0000, 0.4894],\n",
      "        [1.0000, 0.4894],\n",
      "        [1.0000, 0.4894],\n",
      "        [1.0000, 0.4878],\n",
      "        [1.0000, 0.4878],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5116],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5087],\n",
      "        [1.0000, 0.5087],\n",
      "        [1.0000, 0.5087],\n",
      "        [1.0000, 0.5051],\n",
      "        [1.0000, 0.5051],\n",
      "        [1.0000, 0.5012],\n",
      "        [1.0000, 0.5012],\n",
      "        [1.0000, 0.5012],\n",
      "        [1.0000, 0.5005],\n",
      "        [1.0000, 0.5005],\n",
      "        [1.0000, 0.4975],\n",
      "        [1.0000, 0.4975],\n",
      "        [1.0000, 0.4975],\n",
      "        [1.0000, 0.4916],\n",
      "        [1.0000, 0.4916],\n",
      "        [1.0000, 0.5127],\n",
      "        [1.0000, 0.5127],\n",
      "        [1.0000, 0.5127],\n",
      "        [1.0000, 0.5107],\n",
      "        [1.0000, 0.5107],\n",
      "        [1.0000, 0.5080],\n",
      "        [1.0000, 0.5080],\n",
      "        [1.0000, 0.5080],\n",
      "        [1.0000, 0.5076],\n",
      "        [1.0000, 0.5076],\n",
      "        [1.0000, 0.5038],\n",
      "        [1.0000, 0.5038],\n",
      "        [1.0000, 0.5038],\n",
      "        [1.0000, 0.4992],\n",
      "        [1.0000, 0.4992],\n",
      "        [1.0000, 0.4969],\n",
      "        [1.0000, 0.4969],\n",
      "        [1.0000, 0.4969],\n",
      "        [1.0000, 0.4946],\n",
      "        [1.0000, 0.4946],\n",
      "        [1.0000, 0.5134]], dtype=torch.float64)\n",
      "tensor(0.0011, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9964],\n",
      "         [0.4402]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5098]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5078]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4984]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4980]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4964]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4883]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4863]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4774]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4764]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4751]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4743]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4731]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4645]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4628]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4626]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4545]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.4522]],\n",
      "\n",
      "        [[0.9969],\n",
      "         [0.4442]],\n",
      "\n",
      "        [[0.9969],\n",
      "         [0.4444]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.4416]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5018]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4995]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4921]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4902]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4884]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4874]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4860]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4768]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4756]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4750]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4654]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4648]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4565]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4544]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4545]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.4549]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.4529]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5107]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5082]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.5088]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4990]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4987]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4892]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4877]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4867]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4854]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4847]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4746]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4742]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4742]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4635]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4637]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.4526]],\n",
      "\n",
      "        [[0.9978],\n",
      "         [0.4529]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4537]],\n",
      "\n",
      "        [[0.9966],\n",
      "         [0.4420]],\n",
      "\n",
      "        [[0.9967],\n",
      "         [0.4430]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4986]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4984]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4967]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4967]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4953]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4949]]], dtype=torch.float64)\n",
      "tensor([[0.9983, 0.5125],\n",
      "        [1.0096, 0.5205],\n",
      "        [1.0099, 0.5179],\n",
      "        [1.0088, 0.5184],\n",
      "        [1.0083, 0.5201],\n",
      "        [1.0083, 0.5194],\n",
      "        [1.0076, 0.5176],\n",
      "        [1.0074, 0.5175],\n",
      "        [1.0059, 0.5174],\n",
      "        [1.0065, 0.5145],\n",
      "        [1.0065, 0.5136],\n",
      "        [1.0058, 0.5159],\n",
      "        [1.0047, 0.5186],\n",
      "        [1.0033, 0.5172],\n",
      "        [1.0035, 0.5155],\n",
      "        [1.0028, 0.5174],\n",
      "        [1.0039, 0.5043],\n",
      "        [1.0034, 0.5046],\n",
      "        [1.0017, 0.5029],\n",
      "        [1.0017, 0.5034],\n",
      "        [1.0008, 0.5047],\n",
      "        [1.0098, 0.5133],\n",
      "        [1.0094, 0.5144],\n",
      "        [1.0087, 0.5138],\n",
      "        [1.0086, 0.5131],\n",
      "        [1.0085, 0.5120],\n",
      "        [1.0084, 0.5120],\n",
      "        [1.0081, 0.5125],\n",
      "        [1.0069, 0.5121],\n",
      "        [1.0067, 0.5124],\n",
      "        [1.0066, 0.5126],\n",
      "        [1.0053, 0.5110],\n",
      "        [1.0056, 0.5067],\n",
      "        [1.0044, 0.5025],\n",
      "        [1.0039, 0.4991],\n",
      "        [1.0040, 0.5020],\n",
      "        [1.0041, 0.5022],\n",
      "        [1.0036, 0.5032],\n",
      "        [1.0106, 0.5123],\n",
      "        [1.0104, 0.5106],\n",
      "        [1.0104, 0.5125],\n",
      "        [1.0096, 0.5105],\n",
      "        [1.0096, 0.5094],\n",
      "        [1.0087, 0.5082],\n",
      "        [1.0085, 0.5071],\n",
      "        [1.0084, 0.5071],\n",
      "        [1.0083, 0.5078],\n",
      "        [1.0082, 0.5060],\n",
      "        [1.0070, 0.5038],\n",
      "        [1.0070, 0.5062],\n",
      "        [1.0070, 0.5059],\n",
      "        [1.0055, 0.5034],\n",
      "        [1.0055, 0.5045],\n",
      "        [1.0035, 0.5040],\n",
      "        [1.0032, 0.5076],\n",
      "        [1.0035, 0.5060],\n",
      "        [1.0011, 0.5025],\n",
      "        [1.0011, 0.5046],\n",
      "        [1.0094, 0.5131],\n",
      "        [1.0094, 0.5125],\n",
      "        [1.0093, 0.5112],\n",
      "        [1.0093, 0.5112],\n",
      "        [1.0092, 0.5113],\n",
      "        [1.0090, 0.5129]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.5134],\n",
      "        [1.0000, 0.5134],\n",
      "        [1.0000, 0.5109],\n",
      "        [1.0000, 0.5109],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5046],\n",
      "        [1.0000, 0.5046],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5021],\n",
      "        [1.0000, 0.5021],\n",
      "        [1.0000, 0.4977],\n",
      "        [1.0000, 0.4977],\n",
      "        [1.0000, 0.4977],\n",
      "        [1.0000, 0.4964],\n",
      "        [1.0000, 0.4964],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5090],\n",
      "        [1.0000, 0.5090],\n",
      "        [1.0000, 0.5090],\n",
      "        [1.0000, 0.5066],\n",
      "        [1.0000, 0.5066],\n",
      "        [1.0000, 0.5029],\n",
      "        [1.0000, 0.5029],\n",
      "        [1.0000, 0.5029],\n",
      "        [1.0000, 0.5004],\n",
      "        [1.0000, 0.5004],\n",
      "        [1.0000, 0.5004],\n",
      "        [1.0000, 0.5004],\n",
      "        [1.0000, 0.5004],\n",
      "        [1.0000, 0.5155],\n",
      "        [1.0000, 0.5155],\n",
      "        [1.0000, 0.5126],\n",
      "        [1.0000, 0.5126],\n",
      "        [1.0000, 0.5126],\n",
      "        [1.0000, 0.5100],\n",
      "        [1.0000, 0.5100],\n",
      "        [1.0000, 0.5100],\n",
      "        [1.0000, 0.5090],\n",
      "        [1.0000, 0.5090],\n",
      "        [1.0000, 0.5066],\n",
      "        [1.0000, 0.5066],\n",
      "        [1.0000, 0.5049],\n",
      "        [1.0000, 0.5049],\n",
      "        [1.0000, 0.5049],\n",
      "        [1.0000, 0.5030],\n",
      "        [1.0000, 0.5030],\n",
      "        [1.0000, 0.4991],\n",
      "        [1.0000, 0.4991],\n",
      "        [1.0000, 0.4991],\n",
      "        [1.0000, 0.5132],\n",
      "        [1.0000, 0.5132],\n",
      "        [1.0000, 0.5139],\n",
      "        [1.0000, 0.5139],\n",
      "        [1.0000, 0.5139],\n",
      "        [1.0000, 0.5139],\n",
      "        [1.0000, 0.5116]], dtype=torch.float64)\n",
      "tensor(5.2506e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9998],\n",
      "         [0.4854]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4846]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4833]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4736]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4720]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4630]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4630]],\n",
      "\n",
      "        [[0.9985],\n",
      "         [0.4611]],\n",
      "\n",
      "        [[0.9985],\n",
      "         [0.4618]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4630]],\n",
      "\n",
      "        [[0.9975],\n",
      "         [0.4505]],\n",
      "\n",
      "        [[0.9977],\n",
      "         [0.4520]],\n",
      "\n",
      "        [[0.9974],\n",
      "         [0.4496]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5038]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5018]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4929]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4918]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4910]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4808]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4804]],\n",
      "\n",
      "        [[0.9996],\n",
      "         [0.4803]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4786]],\n",
      "\n",
      "        [[0.9995],\n",
      "         [0.4787]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4687]],\n",
      "\n",
      "        [[0.9989],\n",
      "         [0.4670]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.4572]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.4550]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4564]],\n",
      "\n",
      "        [[0.9972],\n",
      "         [0.4471]],\n",
      "\n",
      "        [[0.9969],\n",
      "         [0.4444]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4968]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4948]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4937]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4924]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4924]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4815]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4815]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4817]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4700]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4711]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.4582]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4587]],\n",
      "\n",
      "        [[0.9984],\n",
      "         [0.4601]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.4577]],\n",
      "\n",
      "        [[0.9984],\n",
      "         [0.4598]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5017]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.5008]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4993]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4884]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4872]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4765]],\n",
      "\n",
      "        [[0.9994],\n",
      "         [0.4759]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4741]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4644]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4626]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4637]],\n",
      "\n",
      "        [[0.9988],\n",
      "         [0.4653]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4636]],\n",
      "\n",
      "        [[0.9982],\n",
      "         [0.4577]],\n",
      "\n",
      "        [[0.9984],\n",
      "         [0.4605]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4961]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4941]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4929]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4819]]], dtype=torch.float64)\n",
      "tensor([[1.0079, 0.5128],\n",
      "        [1.0077, 0.5139],\n",
      "        [1.0077, 0.5125],\n",
      "        [1.0066, 0.5107],\n",
      "        [1.0064, 0.5102],\n",
      "        [1.0049, 0.5098],\n",
      "        [1.0049, 0.5097],\n",
      "        [1.0048, 0.5080],\n",
      "        [1.0050, 0.5080],\n",
      "        [1.0051, 0.5084],\n",
      "        [1.0028, 0.5069],\n",
      "        [1.0032, 0.5068],\n",
      "        [1.0029, 0.5045],\n",
      "        [1.0099, 0.5134],\n",
      "        [1.0097, 0.5131],\n",
      "        [1.0089, 0.5123],\n",
      "        [1.0088, 0.5123],\n",
      "        [1.0086, 0.5133],\n",
      "        [1.0076, 0.5110],\n",
      "        [1.0073, 0.5128],\n",
      "        [1.0072, 0.5131],\n",
      "        [1.0072, 0.5117],\n",
      "        [1.0073, 0.5108],\n",
      "        [1.0060, 0.5090],\n",
      "        [1.0059, 0.5078],\n",
      "        [1.0043, 0.5065],\n",
      "        [1.0039, 0.5060],\n",
      "        [1.0042, 0.5063],\n",
      "        [1.0025, 0.4984],\n",
      "        [1.0018, 0.5007],\n",
      "        [1.0094, 0.5096],\n",
      "        [1.0091, 0.5119],\n",
      "        [1.0090, 0.5117],\n",
      "        [1.0089, 0.5117],\n",
      "        [1.0089, 0.5120],\n",
      "        [1.0078, 0.5087],\n",
      "        [1.0078, 0.5091],\n",
      "        [1.0077, 0.5106],\n",
      "        [1.0064, 0.5072],\n",
      "        [1.0065, 0.5076],\n",
      "        [1.0046, 0.5051],\n",
      "        [1.0047, 0.5048],\n",
      "        [1.0049, 0.5052],\n",
      "        [1.0045, 0.5049],\n",
      "        [1.0048, 0.5057],\n",
      "        [1.0098, 0.5123],\n",
      "        [1.0097, 0.5109],\n",
      "        [1.0096, 0.5106],\n",
      "        [1.0086, 0.5091],\n",
      "        [1.0084, 0.5101],\n",
      "        [1.0072, 0.5084],\n",
      "        [1.0072, 0.5083],\n",
      "        [1.0069, 0.5089],\n",
      "        [1.0056, 0.5071],\n",
      "        [1.0053, 0.5054],\n",
      "        [1.0055, 0.5061],\n",
      "        [1.0058, 0.5055],\n",
      "        [1.0055, 0.5060],\n",
      "        [1.0042, 0.5075],\n",
      "        [1.0047, 0.5082],\n",
      "        [1.0092, 0.5131],\n",
      "        [1.0090, 0.5129],\n",
      "        [1.0089, 0.5127],\n",
      "        [1.0077, 0.5110]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.5116],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5092],\n",
      "        [1.0000, 0.5067],\n",
      "        [1.0000, 0.5067],\n",
      "        [1.0000, 0.5067],\n",
      "        [1.0000, 0.5054],\n",
      "        [1.0000, 0.5054],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5040],\n",
      "        [1.0000, 0.5172],\n",
      "        [1.0000, 0.5172],\n",
      "        [1.0000, 0.5150],\n",
      "        [1.0000, 0.5150],\n",
      "        [1.0000, 0.5144],\n",
      "        [1.0000, 0.5144],\n",
      "        [1.0000, 0.5144],\n",
      "        [1.0000, 0.5141],\n",
      "        [1.0000, 0.5141],\n",
      "        [1.0000, 0.5141],\n",
      "        [1.0000, 0.5111],\n",
      "        [1.0000, 0.5111],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5085],\n",
      "        [1.0000, 0.5067],\n",
      "        [1.0000, 0.5067],\n",
      "        [1.0000, 0.5174],\n",
      "        [1.0000, 0.5174],\n",
      "        [1.0000, 0.5174],\n",
      "        [1.0000, 0.5170],\n",
      "        [1.0000, 0.5170],\n",
      "        [1.0000, 0.5160],\n",
      "        [1.0000, 0.5160],\n",
      "        [1.0000, 0.5160],\n",
      "        [1.0000, 0.5145],\n",
      "        [1.0000, 0.5145],\n",
      "        [1.0000, 0.5145],\n",
      "        [1.0000, 0.5145],\n",
      "        [1.0000, 0.5145],\n",
      "        [1.0000, 0.5149],\n",
      "        [1.0000, 0.5149],\n",
      "        [1.0000, 0.5194],\n",
      "        [1.0000, 0.5194],\n",
      "        [1.0000, 0.5194],\n",
      "        [1.0000, 0.5179],\n",
      "        [1.0000, 0.5179],\n",
      "        [1.0000, 0.5182],\n",
      "        [1.0000, 0.5182],\n",
      "        [1.0000, 0.5182],\n",
      "        [1.0000, 0.5187],\n",
      "        [1.0000, 0.5187],\n",
      "        [1.0000, 0.5135],\n",
      "        [1.0000, 0.5135],\n",
      "        [1.0000, 0.5135],\n",
      "        [1.0000, 0.5151],\n",
      "        [1.0000, 0.5151],\n",
      "        [1.0000, 0.5208],\n",
      "        [1.0000, 0.5208],\n",
      "        [1.0000, 0.5208],\n",
      "        [1.0000, 0.5199],\n",
      "        [1.0000, 0.5199]], dtype=torch.float64)\n",
      "tensor(4.8518e-05, grad_fn=<MseLossBackward0>)\n",
      "tensor([[[0.9996],\n",
      "         [0.4812]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4692]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4688]],\n",
      "\n",
      "        [[0.9990],\n",
      "         [0.4692]],\n",
      "\n",
      "        [[0.9980],\n",
      "         [0.4559]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4568]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4583]],\n",
      "\n",
      "        [[0.9981],\n",
      "         [0.4560]],\n",
      "\n",
      "        [[0.9983],\n",
      "         [0.4588]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4986]],\n",
      "\n",
      "        [[1.0000],\n",
      "         [0.4981]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4836]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4827]],\n",
      "\n",
      "        [[0.9997],\n",
      "         [0.4820]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4733]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4716]],\n",
      "\n",
      "        [[0.9992],\n",
      "         [0.4723]],\n",
      "\n",
      "        [[0.9993],\n",
      "         [0.4730]],\n",
      "\n",
      "        [[0.9991],\n",
      "         [0.4708]],\n",
      "\n",
      "        [[0.9986],\n",
      "         [0.4621]],\n",
      "\n",
      "        [[0.9987],\n",
      "         [0.4635]],\n",
      "\n",
      "        [[0.9976],\n",
      "         [0.4507]],\n",
      "\n",
      "        [[0.9979],\n",
      "         [0.4540]],\n",
      "\n",
      "        [[0.9999],\n",
      "         [0.4886]],\n",
      "\n",
      "        [[0.9998],\n",
      "         [0.4876]]], dtype=torch.float64)\n",
      "tensor([[1.0076, 0.5111],\n",
      "        [1.0061, 0.5092],\n",
      "        [1.0060, 0.5091],\n",
      "        [1.0061, 0.5091],\n",
      "        [1.0039, 0.5070],\n",
      "        [1.0044, 0.5036],\n",
      "        [1.0046, 0.5043],\n",
      "        [1.0040, 0.5069],\n",
      "        [1.0045, 0.5069],\n",
      "        [1.0092, 0.5154],\n",
      "        [1.0091, 0.5156],\n",
      "        [1.0077, 0.5132],\n",
      "        [1.0076, 0.5131],\n",
      "        [1.0077, 0.5116],\n",
      "        [1.0066, 0.5102],\n",
      "        [1.0062, 0.5115],\n",
      "        [1.0063, 0.5116],\n",
      "        [1.0064, 0.5117],\n",
      "        [1.0061, 0.5113],\n",
      "        [1.0049, 0.5088],\n",
      "        [1.0051, 0.5090],\n",
      "        [1.0028, 0.5067],\n",
      "        [1.0035, 0.5074],\n",
      "        [1.0083, 0.5132],\n",
      "        [1.0082, 0.5130]], grad_fn=<AddmmBackward0>)\n",
      "tensor([[1.0000, 0.5202],\n",
      "        [1.0000, 0.5202],\n",
      "        [1.0000, 0.5202],\n",
      "        [1.0000, 0.5211],\n",
      "        [1.0000, 0.5211],\n",
      "        [1.0000, 0.5136],\n",
      "        [1.0000, 0.5136],\n",
      "        [1.0000, 0.5136],\n",
      "        [1.0000, 0.5220],\n",
      "        [1.0000, 0.5220],\n",
      "        [1.0000, 0.5215],\n",
      "        [1.0000, 0.5215],\n",
      "        [1.0000, 0.5215],\n",
      "        [1.0000, 0.5203],\n",
      "        [1.0000, 0.5203],\n",
      "        [1.0000, 0.5217],\n",
      "        [1.0000, 0.5217],\n",
      "        [1.0000, 0.5217],\n",
      "        [1.0000, 0.5238],\n",
      "        [1.0000, 0.5238],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5112],\n",
      "        [1.0000, 0.5247],\n",
      "        [1.0000, 0.5247]], dtype=torch.float64)\n",
      "tensor(7.0692e-05, grad_fn=<MseLossBackward0>)\n",
      "Final val loss: 0.005429943478454582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# load file and check MSELoss\n",
    "config_dict = easydict.EasyDict({\n",
    "    \"input_dim\": 32,\n",
    "    \"num_patch\": 36,\n",
    "    \"model_dim\": 32,\n",
    "    \"ffn_dim\": 256,\n",
    "    \"attention_heads\": 4,\n",
    "    \"attention_dropout\": 0.0,\n",
    "    \"dropout\": 0.2,\n",
    "    \"encoder_layers\": 3,\n",
    "    \"decoder_layers\": 3,\n",
    "    \"device\": device,\n",
    "})\n",
    "\n",
    "model = Transformer(config_dict)\n",
    "model.load_state_dict(torch.load('transformer_model.pth', map_location=torch.device('cpu')))\n",
    "model.eval()\n",
    "device = 'cpu'\n",
    "\n",
    "# take world idx 0 as example\n",
    "dataset = KULBarnDataset(df[df['world_idx'] == 0], \"val\")\n",
    "print(len(dataset))\n",
    "loader = DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "final_val_loss = test_model(model, loader, loss_fn)\n",
    "print(\"Final val loss:\", final_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'hydra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[126], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mhydra\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01momegaconf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m OmegaConf\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'hydra'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import hydra\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "import pathlib\n",
    "from torch.utils.data import DataLoader\n",
    "import copy\n",
    "import numpy as np\n",
    "import random\n",
    "import wandb\n",
    "import tqdm\n",
    "import shutil\n",
    "from diffusion_policy.policy.diffusion_unet_lowdim_policy import DiffusionUnetLowdimPolicy\n",
    "from diffusion_policy.workspace.train_diffusion_unet_lowdim_workspace import TrainDiffusionUnetLowdimWorkspace\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'diffusion_policy.dataset'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[164], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdiffusion_policy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase_dataset\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseLowdimDataset\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Dict\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'diffusion_policy.dataset'"
     ]
    }
   ],
   "source": [
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "from typing import Dict\n",
    "import torch\n",
    "import numpy as np\n",
    "import copy\n",
    "from diffusion_policy.common.pytorch_util import dict_apply\n",
    "from diffusion_policy.common.replay_buffer import ReplayBuffer\n",
    "from diffusion_policy.common.sampler import (\n",
    "    SequenceSampler, get_val_mask, downsample_mask)\n",
    "from diffusion_policy.model.common.normalizer import LinearNormalizer\n",
    "from diffusion_policy.dataset.base_dataset import BaseLowdimDataset\n",
    "\n",
    "\n",
    "class KULBarnDiffusionDataset(BaseLowdimDataset):\n",
    "    def __init__(self, df, horizon=4):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.data = df\n",
    "        self.get_local_goal()\n",
    "\n",
    "        self.data = self.data.drop(columns=[\n",
    "            'timestep', 'actual_time', 'optimal_time', \n",
    "            'pos_x', 'pos_y', 'pose_heading', 'goal_x', 'goal_y', 'success'\n",
    "        ])\n",
    "        \n",
    "        self.data = pd.DataFrame(self.data, columns=self.data.columns)\n",
    "        self.horizon = horizon\n",
    "\n",
    "        # Process data columns\n",
    "        self.lidar_cols = [col for col in self.data.columns if 'lidar' in col]\n",
    "        self.actions_cols = [col for col in self.data.columns if 'cmd' in col]\n",
    "        self.non_lidar_cols = [col for col in self.data.columns if col not in self.lidar_cols and col not in self.actions_cols and col != 'world_idx']\n",
    "\n",
    "        self.lidar_data = self.data[self.lidar_cols].values\n",
    "        self.non_lidar_data = self.data[self.non_lidar_cols].values\n",
    "        self.actions_data = self.data[self.actions_cols].values\n",
    "\n",
    "        print(\"Lidar Columns:\", self.lidar_cols)\n",
    "        print(\"Non Lidar Columns:\", self.non_lidar_cols)\n",
    "        print(\"Action Columns:\", self.actions_cols)     \n",
    "\n",
    "        self.grouped_data = self.data.groupby(self.data['world_idx'])\n",
    "        self.horizon = horizon\n",
    "        path_lengths = [len(group) for name, group in self.grouped_data]\n",
    "        self.indices = self.make_indices(path_lengths, horizon)\n",
    "\n",
    "    def get_local_goal(self):\n",
    "        x = self.data['pos_x']\n",
    "        y = self.data['pos_y']\n",
    "        theta = self.data['pose_heading']\n",
    "        goal_x = self.data['goal_x']\n",
    "        goal_y = self.data['goal_y']\n",
    "        self.data['local_x'] = (goal_x - x) * np.cos(theta) + (goal_y - y) * np.sin(theta)\n",
    "        self.data['local_y'] = -(goal_x - x) * np.sin(theta) + (goal_y - y) * np.cos(theta)\n",
    "\n",
    "    def make_indices(self, path_lengths, horizon):\n",
    "        indices = []\n",
    "        for i, path_length in enumerate(path_lengths):\n",
    "            max_start = path_length - horizon\n",
    "            for start in range(max_start):\n",
    "                end = start + horizon\n",
    "                indices.append((i, start, end))\n",
    "        indices = np.array(indices)\n",
    "        return indices\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.indices)\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Dict[str, torch.Tensor]:\n",
    "        idx = self.indices[idx]\n",
    "        start = idx[1]\n",
    "        end = idx[2]\n",
    "\n",
    "        data = {\n",
    "            'obs': self.lidar_data[start:end],\n",
    "            'cond': self.non_lidar_data[start:end],\n",
    "            'action': self.actions_data[start:end],\n",
    "        }\n",
    "        torch_data = dict_apply(data, torch.from_numpy)\n",
    "        return torch_data\n",
    "\n",
    "    def get_normalizer(self, mode='limits', **kwargs):\n",
    "        normalizer = LinearNormalizer()\n",
    "        # train it in using self.data as a dictionary\n",
    "        data_dict = {\n",
    "            'obs': self.lidar_data,\n",
    "            'cond': self.non_lidar_data,\n",
    "            'action': self.actions_data\n",
    "        }\n",
    "        normalizer.fit(data=data_dict, mode=mode, **kwargs)\n",
    "        return normalizer\n",
    "\n",
    "    def get_all_actions(self) -> torch.Tensor:\n",
    "        return torch.from_numpy(self.actions_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lidar Columns: ['lidar_0', 'lidar_1', 'lidar_2', 'lidar_3', 'lidar_4', 'lidar_5', 'lidar_6', 'lidar_7', 'lidar_8', 'lidar_9', 'lidar_10', 'lidar_11', 'lidar_12', 'lidar_13', 'lidar_14', 'lidar_15', 'lidar_16', 'lidar_17', 'lidar_18', 'lidar_19', 'lidar_20', 'lidar_21', 'lidar_22', 'lidar_23', 'lidar_24', 'lidar_25', 'lidar_26', 'lidar_27', 'lidar_28', 'lidar_29', 'lidar_30', 'lidar_31', 'lidar_32', 'lidar_33', 'lidar_34', 'lidar_35', 'lidar_36', 'lidar_37', 'lidar_38', 'lidar_39', 'lidar_40', 'lidar_41', 'lidar_42', 'lidar_43', 'lidar_44', 'lidar_45', 'lidar_46', 'lidar_47', 'lidar_48', 'lidar_49', 'lidar_50', 'lidar_51', 'lidar_52', 'lidar_53', 'lidar_54', 'lidar_55', 'lidar_56', 'lidar_57', 'lidar_58', 'lidar_59', 'lidar_60', 'lidar_61', 'lidar_62', 'lidar_63', 'lidar_64', 'lidar_65', 'lidar_66', 'lidar_67', 'lidar_68', 'lidar_69', 'lidar_70', 'lidar_71', 'lidar_72', 'lidar_73', 'lidar_74', 'lidar_75', 'lidar_76', 'lidar_77', 'lidar_78', 'lidar_79', 'lidar_80', 'lidar_81', 'lidar_82', 'lidar_83', 'lidar_84', 'lidar_85', 'lidar_86', 'lidar_87', 'lidar_88', 'lidar_89', 'lidar_90', 'lidar_91', 'lidar_92', 'lidar_93', 'lidar_94', 'lidar_95', 'lidar_96', 'lidar_97', 'lidar_98', 'lidar_99', 'lidar_100', 'lidar_101', 'lidar_102', 'lidar_103', 'lidar_104', 'lidar_105', 'lidar_106', 'lidar_107', 'lidar_108', 'lidar_109', 'lidar_110', 'lidar_111', 'lidar_112', 'lidar_113', 'lidar_114', 'lidar_115', 'lidar_116', 'lidar_117', 'lidar_118', 'lidar_119', 'lidar_120', 'lidar_121', 'lidar_122', 'lidar_123', 'lidar_124', 'lidar_125', 'lidar_126', 'lidar_127', 'lidar_128', 'lidar_129', 'lidar_130', 'lidar_131', 'lidar_132', 'lidar_133', 'lidar_134', 'lidar_135', 'lidar_136', 'lidar_137', 'lidar_138', 'lidar_139', 'lidar_140', 'lidar_141', 'lidar_142', 'lidar_143', 'lidar_144', 'lidar_145', 'lidar_146', 'lidar_147', 'lidar_148', 'lidar_149', 'lidar_150', 'lidar_151', 'lidar_152', 'lidar_153', 'lidar_154', 'lidar_155', 'lidar_156', 'lidar_157', 'lidar_158', 'lidar_159', 'lidar_160', 'lidar_161', 'lidar_162', 'lidar_163', 'lidar_164', 'lidar_165', 'lidar_166', 'lidar_167', 'lidar_168', 'lidar_169', 'lidar_170', 'lidar_171', 'lidar_172', 'lidar_173', 'lidar_174', 'lidar_175', 'lidar_176', 'lidar_177', 'lidar_178', 'lidar_179', 'lidar_180', 'lidar_181', 'lidar_182', 'lidar_183', 'lidar_184', 'lidar_185', 'lidar_186', 'lidar_187', 'lidar_188', 'lidar_189', 'lidar_190', 'lidar_191', 'lidar_192', 'lidar_193', 'lidar_194', 'lidar_195', 'lidar_196', 'lidar_197', 'lidar_198', 'lidar_199', 'lidar_200', 'lidar_201', 'lidar_202', 'lidar_203', 'lidar_204', 'lidar_205', 'lidar_206', 'lidar_207', 'lidar_208', 'lidar_209', 'lidar_210', 'lidar_211', 'lidar_212', 'lidar_213', 'lidar_214', 'lidar_215', 'lidar_216', 'lidar_217', 'lidar_218', 'lidar_219', 'lidar_220', 'lidar_221', 'lidar_222', 'lidar_223', 'lidar_224', 'lidar_225', 'lidar_226', 'lidar_227', 'lidar_228', 'lidar_229', 'lidar_230', 'lidar_231', 'lidar_232', 'lidar_233', 'lidar_234', 'lidar_235', 'lidar_236', 'lidar_237', 'lidar_238', 'lidar_239', 'lidar_240', 'lidar_241', 'lidar_242', 'lidar_243', 'lidar_244', 'lidar_245', 'lidar_246', 'lidar_247', 'lidar_248', 'lidar_249', 'lidar_250', 'lidar_251', 'lidar_252', 'lidar_253', 'lidar_254', 'lidar_255', 'lidar_256', 'lidar_257', 'lidar_258', 'lidar_259', 'lidar_260', 'lidar_261', 'lidar_262', 'lidar_263', 'lidar_264', 'lidar_265', 'lidar_266', 'lidar_267', 'lidar_268', 'lidar_269', 'lidar_270', 'lidar_271', 'lidar_272', 'lidar_273', 'lidar_274', 'lidar_275', 'lidar_276', 'lidar_277', 'lidar_278', 'lidar_279', 'lidar_280', 'lidar_281', 'lidar_282', 'lidar_283', 'lidar_284', 'lidar_285', 'lidar_286', 'lidar_287', 'lidar_288', 'lidar_289', 'lidar_290', 'lidar_291', 'lidar_292', 'lidar_293', 'lidar_294', 'lidar_295', 'lidar_296', 'lidar_297', 'lidar_298', 'lidar_299', 'lidar_300', 'lidar_301', 'lidar_302', 'lidar_303', 'lidar_304', 'lidar_305', 'lidar_306', 'lidar_307', 'lidar_308', 'lidar_309', 'lidar_310', 'lidar_311', 'lidar_312', 'lidar_313', 'lidar_314', 'lidar_315', 'lidar_316', 'lidar_317', 'lidar_318', 'lidar_319', 'lidar_320', 'lidar_321', 'lidar_322', 'lidar_323', 'lidar_324', 'lidar_325', 'lidar_326', 'lidar_327', 'lidar_328', 'lidar_329', 'lidar_330', 'lidar_331', 'lidar_332', 'lidar_333', 'lidar_334', 'lidar_335', 'lidar_336', 'lidar_337', 'lidar_338', 'lidar_339', 'lidar_340', 'lidar_341', 'lidar_342', 'lidar_343', 'lidar_344', 'lidar_345', 'lidar_346', 'lidar_347', 'lidar_348', 'lidar_349', 'lidar_350', 'lidar_351', 'lidar_352', 'lidar_353', 'lidar_354', 'lidar_355', 'lidar_356', 'lidar_357', 'lidar_358', 'lidar_359', 'lidar_360', 'lidar_361', 'lidar_362', 'lidar_363', 'lidar_364', 'lidar_365', 'lidar_366', 'lidar_367', 'lidar_368', 'lidar_369', 'lidar_370', 'lidar_371', 'lidar_372', 'lidar_373', 'lidar_374', 'lidar_375', 'lidar_376', 'lidar_377', 'lidar_378', 'lidar_379', 'lidar_380', 'lidar_381', 'lidar_382', 'lidar_383', 'lidar_384', 'lidar_385', 'lidar_386', 'lidar_387', 'lidar_388', 'lidar_389', 'lidar_390', 'lidar_391', 'lidar_392', 'lidar_393', 'lidar_394', 'lidar_395', 'lidar_396', 'lidar_397', 'lidar_398', 'lidar_399', 'lidar_400', 'lidar_401', 'lidar_402', 'lidar_403', 'lidar_404', 'lidar_405', 'lidar_406', 'lidar_407', 'lidar_408', 'lidar_409', 'lidar_410', 'lidar_411', 'lidar_412', 'lidar_413', 'lidar_414', 'lidar_415', 'lidar_416', 'lidar_417', 'lidar_418', 'lidar_419', 'lidar_420', 'lidar_421', 'lidar_422', 'lidar_423', 'lidar_424', 'lidar_425', 'lidar_426', 'lidar_427', 'lidar_428', 'lidar_429', 'lidar_430', 'lidar_431', 'lidar_432', 'lidar_433', 'lidar_434', 'lidar_435', 'lidar_436', 'lidar_437', 'lidar_438', 'lidar_439', 'lidar_440', 'lidar_441', 'lidar_442', 'lidar_443', 'lidar_444', 'lidar_445', 'lidar_446', 'lidar_447', 'lidar_448', 'lidar_449', 'lidar_450', 'lidar_451', 'lidar_452', 'lidar_453', 'lidar_454', 'lidar_455', 'lidar_456', 'lidar_457', 'lidar_458', 'lidar_459', 'lidar_460', 'lidar_461', 'lidar_462', 'lidar_463', 'lidar_464', 'lidar_465', 'lidar_466', 'lidar_467', 'lidar_468', 'lidar_469', 'lidar_470', 'lidar_471', 'lidar_472', 'lidar_473', 'lidar_474', 'lidar_475', 'lidar_476', 'lidar_477', 'lidar_478', 'lidar_479', 'lidar_480', 'lidar_481', 'lidar_482', 'lidar_483', 'lidar_484', 'lidar_485', 'lidar_486', 'lidar_487', 'lidar_488', 'lidar_489', 'lidar_490', 'lidar_491', 'lidar_492', 'lidar_493', 'lidar_494', 'lidar_495', 'lidar_496', 'lidar_497', 'lidar_498', 'lidar_499', 'lidar_500', 'lidar_501', 'lidar_502', 'lidar_503', 'lidar_504', 'lidar_505', 'lidar_506', 'lidar_507', 'lidar_508', 'lidar_509', 'lidar_510', 'lidar_511', 'lidar_512', 'lidar_513', 'lidar_514', 'lidar_515', 'lidar_516', 'lidar_517', 'lidar_518', 'lidar_519', 'lidar_520', 'lidar_521', 'lidar_522', 'lidar_523', 'lidar_524', 'lidar_525', 'lidar_526', 'lidar_527', 'lidar_528', 'lidar_529', 'lidar_530', 'lidar_531', 'lidar_532', 'lidar_533', 'lidar_534', 'lidar_535', 'lidar_536', 'lidar_537', 'lidar_538', 'lidar_539', 'lidar_540', 'lidar_541', 'lidar_542', 'lidar_543', 'lidar_544', 'lidar_545', 'lidar_546', 'lidar_547', 'lidar_548', 'lidar_549', 'lidar_550', 'lidar_551', 'lidar_552', 'lidar_553', 'lidar_554', 'lidar_555', 'lidar_556', 'lidar_557', 'lidar_558', 'lidar_559', 'lidar_560', 'lidar_561', 'lidar_562', 'lidar_563', 'lidar_564', 'lidar_565', 'lidar_566', 'lidar_567', 'lidar_568', 'lidar_569', 'lidar_570', 'lidar_571', 'lidar_572', 'lidar_573', 'lidar_574', 'lidar_575', 'lidar_576', 'lidar_577', 'lidar_578', 'lidar_579', 'lidar_580', 'lidar_581', 'lidar_582', 'lidar_583', 'lidar_584', 'lidar_585', 'lidar_586', 'lidar_587', 'lidar_588', 'lidar_589', 'lidar_590', 'lidar_591', 'lidar_592', 'lidar_593', 'lidar_594', 'lidar_595', 'lidar_596', 'lidar_597', 'lidar_598', 'lidar_599', 'lidar_600', 'lidar_601', 'lidar_602', 'lidar_603', 'lidar_604', 'lidar_605', 'lidar_606', 'lidar_607', 'lidar_608', 'lidar_609', 'lidar_610', 'lidar_611', 'lidar_612', 'lidar_613', 'lidar_614', 'lidar_615', 'lidar_616', 'lidar_617', 'lidar_618', 'lidar_619', 'lidar_620', 'lidar_621', 'lidar_622', 'lidar_623', 'lidar_624', 'lidar_625', 'lidar_626', 'lidar_627', 'lidar_628', 'lidar_629', 'lidar_630', 'lidar_631', 'lidar_632', 'lidar_633', 'lidar_634', 'lidar_635', 'lidar_636', 'lidar_637', 'lidar_638', 'lidar_639', 'lidar_640', 'lidar_641', 'lidar_642', 'lidar_643', 'lidar_644', 'lidar_645', 'lidar_646', 'lidar_647', 'lidar_648', 'lidar_649', 'lidar_650', 'lidar_651', 'lidar_652', 'lidar_653', 'lidar_654', 'lidar_655', 'lidar_656', 'lidar_657', 'lidar_658', 'lidar_659', 'lidar_660', 'lidar_661', 'lidar_662', 'lidar_663', 'lidar_664', 'lidar_665', 'lidar_666', 'lidar_667', 'lidar_668', 'lidar_669', 'lidar_670', 'lidar_671', 'lidar_672', 'lidar_673', 'lidar_674', 'lidar_675', 'lidar_676', 'lidar_677', 'lidar_678', 'lidar_679', 'lidar_680', 'lidar_681', 'lidar_682', 'lidar_683', 'lidar_684', 'lidar_685', 'lidar_686', 'lidar_687', 'lidar_688', 'lidar_689', 'lidar_690', 'lidar_691', 'lidar_692', 'lidar_693', 'lidar_694', 'lidar_695', 'lidar_696', 'lidar_697', 'lidar_698', 'lidar_699', 'lidar_700', 'lidar_701', 'lidar_702', 'lidar_703', 'lidar_704', 'lidar_705', 'lidar_706', 'lidar_707', 'lidar_708', 'lidar_709', 'lidar_710', 'lidar_711', 'lidar_712', 'lidar_713', 'lidar_714', 'lidar_715', 'lidar_716', 'lidar_717', 'lidar_718', 'lidar_719']\n",
      "Non Lidar Columns: ['twist_linear', 'twist_angular', 'local_x', 'local_y']\n",
      "Action Columns: ['cmd_vel_linear', 'cmd_vel_angular']\n",
      "141127\n"
     ]
    }
   ],
   "source": [
    "train_dataset = KULBarnDiffusionDataset(train_df)\n",
    "train_dataloader = DataLoader(train_dataset)\n",
    "normalizer = train_dataset.get_normalizer()\n",
    "print(len(train_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 4, 720])\n",
      "torch.Size([1, 4, 4])\n",
      "torch.Size([1, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    # print(batch)\n",
    "    print(batch['obs'].shape)\n",
    "    print(batch['cond'].shape)\n",
    "    print(batch['action'].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from diffusion_policy.policy.diffusion_unet_lidar_policy import DiffusionUnetLidarPolicy\n",
    "from diffusion_policy.model.diffusion.conditional_unet1d import ConditionalUnet1D\n",
    "from diffusers.schedulers.scheduling_ddpm import DDPMScheduler\n",
    "\n",
    "obs_dim = batch['obs'].shape[-1]\n",
    "action_dim = batch['action'].shape[-1]\n",
    "input_dim = obs_dim + action_dim\n",
    "model = ConditionalUnet1D(input_dim=input_dim)\n",
    "noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='linear')\n",
    "horizon = 4\n",
    "policy = DiffusionUnetLidarPolicy(\n",
    "    model=model, \n",
    "    noise_scheduler=noise_scheduler, \n",
    "    horizon=horizon, \n",
    "    obs_dim=obs_dim, \n",
    "    action_dim=action_dim, \n",
    "    n_obs_steps=4,\n",
    "    n_action_steps=4,\n",
    "    pred_action_steps_only=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy.set_normalizer(normalizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/141127 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Tensors must have same number of dimensions: got 2 and 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(NUM_EPOCHS):\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m tqdm(train_dataloader):\n\u001b[0;32m---> 13\u001b[0m         loss \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     15\u001b[0m         loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/raid/joshua/codes/mlda-barn-2024/train_imitation/diffusion_policy/diffusion_policy/policy/diffusion_unet_lidar_policy.py:242\u001b[0m, in \u001b[0;36mDiffusionUnetLidarPolicy.compute_loss\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    239\u001b[0m noisy_trajectory[condition_mask] \u001b[38;5;241m=\u001b[39m trajectory[condition_mask]\n\u001b[1;32m    241\u001b[0m \u001b[38;5;66;03m# Predict the noise residual, passing 'cond' as an additional conditioning input\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnoisy_trajectory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m                \u001b[49m\u001b[43mlocal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal_cond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_cond\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mglobal_cond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    245\u001b[0m pred_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoise_scheduler\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprediction_type\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pred_type \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon\u001b[39m\u001b[38;5;124m'\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/robodiff/lib/python3.9/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m/raid/joshua/codes/mlda-barn-2024/train_imitation/diffusion_policy/diffusion_policy/model/diffusion/conditional_unet1d.py:199\u001b[0m, in \u001b[0;36mConditionalUnet1D.forward\u001b[0;34m(self, sample, timestep, local_cond, global_cond, **kwargs)\u001b[0m\n\u001b[1;32m    196\u001b[0m global_feature \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdiffusion_step_encoder(timesteps)\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m global_cond \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 199\u001b[0m     global_feature \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\n\u001b[1;32m    200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mglobal_feature\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mglobal_cond\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;66;03m# encode local features\u001b[39;00m\n\u001b[1;32m    204\u001b[0m h_local \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Tensors must have same number of dimensions: got 2 and 3"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "NUM_EPOCHS = 10\n",
    "losses = []\n",
    "save_loss_every = 1000\n",
    "total_loss = 0\n",
    "count = 0\n",
    "\n",
    "optimizer = optim.Adam(policy.model.parameters(), lr=5e-5)\n",
    "policy.model.train()\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    for batch in tqdm(train_dataloader):\n",
    "        loss = policy.compute_loss(batch)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        count += 1\n",
    "        if count >= save_loss_every:\n",
    "            curr_loss = total_loss / save_loss_every\n",
    "            print(\"Loss:\", curr_loss)\n",
    "            losses.append(curr_loss)\n",
    "            total_loss = 0\n",
    "            count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot loss\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(losses)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
